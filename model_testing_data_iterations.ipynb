{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import farmhash # https://github.com/veelion/python-farmhash\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "def k_folds_partition(data, hash_column = 'text', n_splits = 10, seed = 43):\n",
    "    set_seed(seed)\n",
    "    data = data.copy()\n",
    "    partition_hash = data[hash_column].apply(lambda x: farmhash.hash64withseed(x, seed))\n",
    "    partition = np.abs(partition_hash % n_splits)\n",
    "    return partition\n",
    "\n",
    "def remove_tabs_newlines(x):\n",
    "    return re.sub(r\"[\\n\\t\\r]*\", \"\", x)\n",
    "def remove_multi_spaces(x):\n",
    "    return re.sub(r\"\\s\\s+\", \" \", x)\n",
    "def remove_punctuation(text):\n",
    "    output_text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../311_text_classifier/src\")\n",
    "from models.evaluate import get_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"max_rows\", 999)\n",
    "pd.set_option(\"max_columns\", 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 43\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep data fon D2 task\n",
    "\n",
    "# d1_files = [\n",
    "#     \"/hub/data_unprocessed/311/ManholeComplainD2POC.csv\",\n",
    "#     \"/hub/311_text_classifier/data/raw/Qualifiers.csv\"\n",
    "# ]\n",
    "\n",
    "# qualifiers = {\n",
    "#     'Sign Types':['Stop Sign', 'Yield Sign', 'One Way Sign', 'Parking Sign',\n",
    "#                   'Speed Limit Sign','Pedestrian Sign','Unknown Sign',\n",
    "#                   'Advance Traffic Control Sign','Public Safety Sign',\n",
    "#                   'Street Name Sign','Unspecified Sign','Other Sign'],\n",
    "#     'Locations':['Unspecified Location','In Ditch','On Sidewalk','In Road',\n",
    "#                  'In Traffic Lane','In Park','Path','In Yard','In Water',\n",
    "#                  'Roadside','Parking Lot','On Parking Strip','In Median',\n",
    "#                  'On Private Property','On Shoulder','In Bike Lane','In Street Gutter',\n",
    "#                  'Sight Obstruction','In Building','In Driveway','Unknown Location',\n",
    "#                  'Alley','Vacant Lot','Trail','Curb'],\n",
    "#     'Hazards':['Ice Hazard','Electrical Hazard'],\n",
    "#     'Graffiti Tags':['Gang','Ideological'],\n",
    "#     'Vehicle Types':['VehicleType:Bicycle'],\n",
    "#     'Manhole Complaints':['Cover Missing', 'Sunken Manhole', 'Displaced Cover',\n",
    "#                           'Damaged Cover', 'Concrete Issue', 'Insecure Cover',\n",
    "#                           'Raised Manhole', 'Structure Damage', 'Noisy Cover']\n",
    "# }\n",
    "# qualifier_map = {v:k for k in qualifiers.keys() for v in qualifiers[k]}\n",
    "\n",
    "# data = []\n",
    "# for file in d1_files:\n",
    "#     temp=pd.read_csv(file)\n",
    "#     temp.columns = ['text', 'category']\n",
    "#     data.append(temp)\n",
    "# data = pd.concat(data, axis=0)\n",
    "# data['daupler_generated'] = np.nan\n",
    "# data['qualifier'] = data['category'].map(qualifier_map)\n",
    "\n",
    "# data.to_csv(f\"/hub/311_text_classifier/data/raw/PW-D2-{date_tag}-PROD.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate D2 data into individual qualifier files\n",
    "\n",
    "# task = 'D2'\n",
    "# date_tag = \"2021_04_08\"\n",
    "# file = f\"/hub/311_text_classifier/data/raw/PW-{task}-{date_tag}-PROD.csv\"\n",
    "\n",
    "# # Break D2 taks into files by qualifier\n",
    "# data = pd.read_csv(file)\n",
    "\n",
    "# d2 = {\n",
    "#     'Manhole Complaints':'MANC',\n",
    "#     'Locations':'LOC', \n",
    "#     'Hazards':'HAZ', \n",
    "#     'Sign Types':'SIGNT', \n",
    "#     'Graffiti Tags':'GTAG', \n",
    "#     'Vehicle Types':'VEHT'\n",
    "# }\n",
    "\n",
    "# qual = {}\n",
    "# for qualifier in data.qualifier.unique():\n",
    "#     qual[d2[qualifier]] = data[data['qualifier']==qualifier].copy().reset_index(drop=True)\n",
    "    \n",
    "# for key in qual.keys():\n",
    "#     print(key)\n",
    "#     print(qual[key].shape)\n",
    "#     print(qual[key]['category'].nunique())\n",
    "#     display(qual[key].head(1))\n",
    "    \n",
    "    \n",
    "\n",
    "# # dict_keys(['MANC', 'LOC', 'HAZ', 'SIGNT', 'GTAG', 'VEHT'])\n",
    "    \n",
    "# for key in qual.keys():\n",
    "#     task = key\n",
    "#     file = f\"/hub/311_text_classifier/data/raw/PW-{task}-{date_tag}-PROD.csv\"\n",
    "#     print(file)\n",
    "#     qual[key].to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data with punctuation removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = ['D0', 'D1', 'D2', 'MANC', 'LOC', 'HAZ', 'SIGNT', 'GTAG', 'VEHT']\n",
    "fold = 8\n",
    "date_tag = \"2021_04_08\"\n",
    "\n",
    "for task in tasks:\n",
    "    data_dir = f\"/hub/CA-MTL/data/{task}\"\n",
    "    file = f\"/hub/311_text_classifier/data/raw/PW-{task}-{date_tag}-PROD.csv\"\n",
    "\n",
    "    out_dir = f\"{data_dir}/{date_tag}_WO_PUNCT\"\n",
    "    train_file_out = f\"{out_dir}/train.tsv\"\n",
    "    train_dev_file_out = f\"{out_dir}/train-dev.tsv\"\n",
    "    dev_file_out = f\"{out_dir}/dev.tsv\"\n",
    "    test_file_out = f\"{out_dir}/test.tsv\"\n",
    "    metadata_file_out = f\"{out_dir}/metadata.json\"\n",
    "\n",
    "    metadata = dict(\n",
    "        raw_data_file = file, \n",
    "        data_version = date_tag, \n",
    "        task_name = task, \n",
    "        file_paths = {\n",
    "            'train':train_file_out, \n",
    "            'train-dev':train_dev_file_out, \n",
    "            'dev':dev_file_out, \n",
    "            'test':test_file_out\n",
    "        },\n",
    "        partition_rules = [\n",
    "            f'external/daupler together; 10-fold split; seed = {seed}; selected fold = {fold}',\n",
    "            f'selected fold used for train_dev, dev, and test as placeholders'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        os.mkdir(out_dir)\n",
    "    except OSError as error:\n",
    "        print(\"Directory already exists\")\n",
    "        pass\n",
    "\n",
    "    # Read data and remove all tabs, multi-spaces, and new lines\n",
    "    data = pd.read_csv(file)\n",
    "\n",
    "    data['text'] = data['text'].apply(remove_tabs_newlines)\n",
    "    data['text'] = data['text'].apply(remove_multi_spaces)\n",
    "    data['text'] = data['text'].apply(remove_punctuation)\n",
    "    data = data.drop_duplicates('text').reset_index(drop=True)\n",
    "\n",
    "    # Handle remapping tasks\n",
    "    if task == 'D0':\n",
    "        data = data[data['category'] != 'Fire'].reset_index(drop=True)\n",
    "        data['category'] = np.where(\n",
    "            data['category']=='Parks', 'Defer', data['category'])\n",
    "    elif task == 'D1':\n",
    "        data['category'] = np.where(\n",
    "            data['category']=='Water Meter Issue', 'Meter Issue', data['category'])   \n",
    "    \n",
    "    #Partition External Data in Train and Train-Dev\n",
    "    data['partition'] = k_folds_partition(\n",
    "        data, hash_column = 'text', n_splits = 10, seed = seed)\n",
    "\n",
    "    train_condition = data['partition']!=fold\n",
    "    train = data[train_condition].reset_index(drop=True)\n",
    "    train_dev = data[~train_condition].reset_index(drop=True)\n",
    "    dev = data[~train_condition].reset_index(drop=True)\n",
    "    test = data[~train_condition].reset_index(drop=True)\n",
    "\n",
    "    #Generate Metadata\n",
    "    metadata['labels'] = data['category'].sort_values().unique().tolist()\n",
    "\n",
    "    out_cols = {\n",
    "        'D0':[\n",
    "            'text',\n",
    "            'category',\n",
    "            'internal_id',\n",
    "            'external_id'\n",
    "        ],\n",
    "        'D1':[\n",
    "            'text',\n",
    "            'category',\n",
    "            'internal_id',\n",
    "            'external_id'\n",
    "        ],\n",
    "        'D2':[\n",
    "            'text',\n",
    "            'category',\n",
    "            'qualifier'\n",
    "        ],\n",
    "        'MANC':[\n",
    "            'text',\n",
    "            'category',\n",
    "            'qualifier'\n",
    "        ],\n",
    "        'LOC':[\n",
    "            'text',\n",
    "            'category',\n",
    "            'qualifier'\n",
    "        ],\n",
    "        'HAZ':[\n",
    "            'text',\n",
    "            'category',\n",
    "            'qualifier'\n",
    "        ],\n",
    "        'SIGNT':[\n",
    "            'text',\n",
    "            'category',\n",
    "            'qualifier'\n",
    "        ],\n",
    "        'GTAG':[\n",
    "            'text',\n",
    "            'category',\n",
    "            'qualifier'\n",
    "        ],\n",
    "        'VEHT':[\n",
    "            'text',\n",
    "            'category',\n",
    "            'qualifier'\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Write data\n",
    "    train[out_cols[task]].to_csv(train_file_out,sep='\\t',index=False)\n",
    "    train_dev[out_cols[task]].to_csv(train_dev_file_out,sep='\\t',index=False)\n",
    "    dev[out_cols[task]].to_csv(dev_file_out,sep='\\t',index=False)\n",
    "    test[out_cols[task]].to_csv(test_file_out,sep='\\t',index=False)\n",
    "    json.dump(metadata, open(metadata_file_out, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "python run.py \\\n",
    "--model_name_or_path CA-MTL-tiny \\\n",
    "--data_dir /hub/CA-MTL/data \\\n",
    "--output_dir /hub/CA-MTL/mock_models \\\n",
    "--tasks D0 D1 MANC LOC SIGNT \\\n",
    "--overwrite_cache \\\n",
    "--task_data_folders D0/2021_04_08_WO_PUNCT D1/2021_04_08_WO_PUNCT MANC/2021_04_08_WO_PUNCT LOC/2021_04_08_WO_PUNCT SIGNT/2021_04_08_WO_PUNCT \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_predict \\\n",
    "--evaluate_during_training \\\n",
    "--per_device_train_batch_size 32 \\\n",
    "--per_device_eval_batch_size 32 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--adam_epsilon 1e-8 \\\n",
    "--num_train_epochs 7 \\\n",
    "--warmup_steps 0 \\\n",
    "--save_steps 1500 \\\n",
    "--save_total_limit 1 \\\n",
    "--seed 43\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional task data\n",
    "- Water\n",
    "- Sewer\n",
    "- Defer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = ['Water', 'Sewer', 'Defer', 'Code Enforcement']\n",
    "fold = 8\n",
    "date_tag = \"2021_04_08\"\n",
    "parent_files = []\n",
    "\n",
    "# Read D1 data anf process\n",
    "high_task = 'D0'\n",
    "file = f\"/hub/311_text_classifier/data/raw/PW-{high_task}-{date_tag}-PROD.csv\"\n",
    "parent_files.append(file)\n",
    "data = pd.read_csv(file)\n",
    "\n",
    "data['text'] = data['text'].apply(remove_tabs_newlines)\n",
    "data['text'] = data['text'].apply(remove_multi_spaces)\n",
    "data = data.drop_duplicates('text').reset_index(drop=True)\n",
    "\n",
    "# Handle remapping tasks\n",
    "if high_task == 'D0':\n",
    "    data = data[data['category'] != 'Fire'].reset_index(drop=True)\n",
    "    data['category'] = np.where(\n",
    "        data['category']=='Parks', 'Defer', data['category'])\n",
    "\n",
    "# Break data up into sub-tasks\n",
    "cat_data = {}\n",
    "for cat in tasks:\n",
    "    cat_data[cat] = data[data.category == cat].copy().reset_index(drop=True)\n",
    "    print(f\"{cat}: {cat_data[cat].shape}\")\n",
    "\n",
    "# Read D1 data and process\n",
    "high_task = 'D1'\n",
    "file = f\"/hub/311_text_classifier/data/raw/PW-{high_task}-{date_tag}-PROD.csv\"\n",
    "parent_files.append(file)\n",
    "\n",
    "# Read data and remove all tabs, multi-spaces, and new lines\n",
    "data = pd.read_csv(file)\n",
    "\n",
    "data['text'] = data['text'].apply(remove_tabs_newlines)\n",
    "data['text'] = data['text'].apply(remove_multi_spaces)\n",
    "data = data.drop_duplicates('text').reset_index(drop=True)\n",
    "\n",
    "#Partition Data\n",
    "data['partition'] = k_folds_partition(\n",
    "    data, hash_column = 'text', n_splits = 10, seed = seed)\n",
    "\n",
    "# Handle remapping tasks\n",
    "if high_task == 'D1':\n",
    "    data['category'] = np.where(\n",
    "        data['category']=='Water Meter Issue', 'Meter Issue', data['category'])   \n",
    "\n",
    "# Select data for task based on D0 groupings\n",
    "d1_cat_data = {}\n",
    "for cat in tasks:\n",
    "    cat_text = cat_data[cat].text.to_list()\n",
    "    d1_cat_data[cat] = data[data.text.isin(cat_text)]\n",
    "\n",
    "metadata_review = []\n",
    "# Write out new tasks\n",
    "for task in tasks:\n",
    "    data_dir = f\"/hub/CA-MTL/data/{task.upper().replace(' ', '')}\"\n",
    "\n",
    "    out_dir = f\"{data_dir}/{date_tag}\"\n",
    "    train_file_out = f\"{out_dir}/train.tsv\"\n",
    "    train_dev_file_out = f\"{out_dir}/train-dev.tsv\"\n",
    "    dev_file_out = f\"{out_dir}/dev.tsv\"\n",
    "    test_file_out = f\"{out_dir}/test.tsv\"\n",
    "    metadata_file_out = f\"{out_dir}/metadata.json\"\n",
    "\n",
    "    metadata = dict(\n",
    "        raw_data_file = parent_files, \n",
    "        data_version = date_tag, \n",
    "        task_name = task.upper().replace(' ', ''), \n",
    "        file_paths = {\n",
    "            'train':train_file_out, \n",
    "            'train-dev':train_dev_file_out, \n",
    "            'dev':dev_file_out, \n",
    "            'test':test_file_out\n",
    "        },\n",
    "        partition_rules = [\n",
    "            f'external/daupler together; 10-fold split; seed = {seed}; selected fold = {fold}',\n",
    "            f'selected fold used for train_dev, dev, and test as placeholders',\n",
    "            f'task category selected from full dataset'\n",
    "        ]\n",
    "    )\n",
    "    metadata_review.append(metadata)\n",
    "\n",
    "    try:\n",
    "        os.mkdir(data_dir)\n",
    "    except OSError as error:\n",
    "        print(\"Directory already exists\")\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(out_dir)\n",
    "    except OSError as error:\n",
    "        print(\"Directory already exists\")\n",
    "        pass\n",
    "\n",
    "\n",
    "    train_condition = d1_cat_data[task]['partition']!=fold\n",
    "    train = d1_cat_data[task][train_condition].reset_index(drop=True)\n",
    "    train_dev = d1_cat_data[task][~train_condition].reset_index(drop=True)\n",
    "    dev = d1_cat_data[task][~train_condition].reset_index(drop=True)\n",
    "    test = d1_cat_data[task][~train_condition].reset_index(drop=True)\n",
    "\n",
    "    #Generate Metadata\n",
    "    metadata['labels'] = d1_cat_data[task]['category'].sort_values().unique().tolist()\n",
    "\n",
    "    out_cols = ['text',\n",
    "                'category',\n",
    "                'internal_id',\n",
    "                'external_id']\n",
    "    \n",
    "    # Write data\n",
    "    train[out_cols].to_csv(train_file_out,sep='\\t',index=False)\n",
    "    train_dev[out_cols].to_csv(train_dev_file_out,sep='\\t',index=False)\n",
    "    dev[out_cols].to_csv(dev_file_out,sep='\\t',index=False)\n",
    "    test[out_cols].to_csv(test_file_out,sep='\\t',index=False)\n",
    "    json.dump(metadata, open(metadata_file_out, 'w'))\n",
    "    \n",
    "for i in range(len(metadata_review)):\n",
    "    print(metadata_review[i]['task_name'])\n",
    "    print(len(metadata_review[i]['labels']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "python run.py \\\n",
    "--model_name_or_path CA-MTL-tiny \\\n",
    "--data_dir /hub/CA-MTL/data \\\n",
    "--output_dir /hub/CA-MTL/mock_models \\\n",
    "--tasks D0 D1 MANC LOC SIGNT WATER SEWER DEFER CODEENFORCEMENT \\\n",
    "--overwrite_cache \\\n",
    "--task_data_folders D0/2021_04_08 D1/2021_04_08 MANC/2021_04_08 LOC/2021_04_08 SIGNT/2021_04_08 WATER/2021_04_08 SEWER/2021_04_08 DEFER/2021_04_08 CODEENFORCEMENT/2021_04_08  \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_predict \\\n",
    "--evaluate_during_training \\\n",
    "--per_device_train_batch_size 32 \\\n",
    "--per_device_eval_batch_size 32 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--adam_epsilon 1e-8 \\\n",
    "--num_train_epochs 7 \\\n",
    "--warmup_steps 0 \\\n",
    "--save_steps 1500 \\\n",
    "--save_total_limit 1 \\\n",
    "--seed 43\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "python run.py \\\n",
    "--model_name_or_path CA-MTL-tiny \\\n",
    "--data_dir /hub/CA-MTL/data \\\n",
    "--output_dir /hub/CA-MTL/mock_models \\\n",
    "--tasks D0 D1 MANC LOC SIGNT \\\n",
    "--overwrite_cache \\\n",
    "--task_data_folders D0/2021_04_08_WO_PUNCT D1/2021_04_08_WO_PUNCT MANC/2021_04_08_WO_PUNCT LOC/2021_04_08_WO_PUNCT SIGNT/2021_04_08_WO_PUNCT \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_predict \\\n",
    "--evaluate_during_training \\\n",
    "--per_device_train_batch_size 32 \\\n",
    "--per_device_eval_batch_size 32 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--adam_epsilon 1e-8 \\\n",
    "--num_train_epochs 7 \\\n",
    "--warmup_steps 0 \\\n",
    "--save_steps 1500 \\\n",
    "--save_total_limit 1 \\\n",
    "--seed 43\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "python run_inference.py \\\n",
    "--model_name_or_path /hub/CA-MTL/mock_models/vital-smoke-40-9000 \\\n",
    "--data_dir /hub/CA-MTL/data \\\n",
    "--output_dir /hub/CA-MTL/data/SCORED \\\n",
    "--overwrite_cache \\\n",
    "--task_data_folders TOSCORE \\\n",
    "--do_predict\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move dev set to output folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate 10-Fold Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up dictionary of files for data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/hub/CA-MTL/mock_models/\"\n",
    "runs = [\n",
    "    'vibrant-river-1',\n",
    "    'colorful-elevator-2',\n",
    "    'super-deluge-3',\n",
    "    'elated-wave-5',\n",
    "    'magic-lake-6',\n",
    "    'neat-wind-7',\n",
    "    'faithful-music-8',\n",
    "    'winter-bush-9',\n",
    "    'stellar-paper-10',\n",
    "    'still-glade-11'\n",
    "]\n",
    "files = os.listdir(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_files = defaultdict(defaultdict)\n",
    "for run in runs:\n",
    "    for d in ['D0', 'D1']:\n",
    "        run_files[run][d] = {}\n",
    "        for partition in ['test', 'dev']:\n",
    "            run_files[run][d][partition] = [\n",
    "                file for file in files if (run in file) & file.startswith(f\"{d}_{partition}\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data for each partition run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data = defaultdict(defaultdict)\n",
    "for run in runs:\n",
    "    for d in ['D0', 'D1']:\n",
    "        data = pd.read_csv(\n",
    "            f\"{model_dir}/{run_files[run][d]['dev'][0]}\", sep=\"\\t\")\n",
    "        data['prediction'] = pd.read_csv(\n",
    "            f\"{model_dir}/{run_files[run][d]['test'][0]}\", sep=\"\\t\")['prediction']\n",
    "        data['run'] = run\n",
    "        run_data[run][d] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append data into a single frame for metric analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0_data = pd.DataFrame()\n",
    "d1_data = pd.DataFrame()\n",
    "for run in run_data.keys():\n",
    "    d0_data = d0_data.append(run_data[run]['D0'], ignore_index = True)\n",
    "    d1_data = d1_data.append(run_data[run]['D1'], ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate metrics across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "metrics['D0'] = get_metrics(\n",
    "    data=d0_data, true_y=\"category\", predicted_y=f\"prediction\", \n",
    "            labels=d0_data[\"category\"].unique().tolist())\n",
    "metrics['D1'] = get_metrics(\n",
    "    data=d1_data, true_y=\"category\", predicted_y=f\"prediction\", \n",
    "            labels=d1_data[\"category\"].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics['D0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics['D1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate metrics by fold and plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_metrics = {}\n",
    "run_metrics['D0'] = {}\n",
    "run_metrics['D1'] = {}\n",
    "for run in runs:\n",
    "    d0_temp = d0_data[d0_data['run']==run].copy()\n",
    "    d1_temp = d1_data[d1_data['run']==run].copy()\n",
    "    \n",
    "    run_metrics['D0'][run] = get_metrics(\n",
    "        data=d0_temp, true_y=\"category\", predicted_y=f\"prediction\", \n",
    "        labels=d0_temp[\"category\"].unique().tolist())\n",
    "    run_metrics['D1'][run] = get_metrics(\n",
    "        data=d1_temp, true_y=\"category\", predicted_y=f\"prediction\", \n",
    "        labels=d1_temp[\"category\"].unique().tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics = pd.DataFrame()\n",
    "d0_f1, d1_f1 = [], []\n",
    "for run in runs:\n",
    "    d0_f1.append(run_metrics['D0'][run].loc[0, 'f1'])\n",
    "    d1_f1.append(run_metrics['D1'][run].loc[0, 'f1'])\n",
    "\n",
    "temp = pd.DataFrame({\n",
    "    \"wghtd_f1\":d0_f1+d1_f1, \n",
    "    \"D\":[f\"D0\"]*len(runs) + [f\"D1\"]*len(runs)})\n",
    "plot_metrics = plot_metrics.append(temp, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(15, 8))\n",
    "f.suptitle(f\"Weighted F1 Performance on Daupler vs External Data\\n(CA-MTL: tinyBERT)\", \n",
    "          fontsize=18)\n",
    "gs = f.add_gridspec(1, 1)\n",
    "f.patch.set_facecolor('white')\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    ax = f.add_subplot(gs[0, 0])\n",
    "    sns.violinplot(x=\"D\", y=\"wghtd_f1\", data=plot_metrics)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate metrics by fold and source and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0_data['daupler_generated'] = np.where(\n",
    "    d0_data['external_id'].str.contains('daupler'), 1, 0)\n",
    "d1_data['daupler_generated'] = np.where(\n",
    "    d1_data['external_id'].str.contains('daupler'), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_metrics = {}\n",
    "source_metrics['D0'] = defaultdict(defaultdict)\n",
    "source_metrics['D1'] = defaultdict(defaultdict)\n",
    "for run in runs:\n",
    "    d0_dau = d0_data[\n",
    "        (d0_data['run']==run) & (d0_data['daupler_generated']==1)].copy()\n",
    "    d0_ext = d0_data[\n",
    "        (d0_data['run']==run) & (d0_data['daupler_generated']!=1)].copy()\n",
    "    d1_dau = d1_data[\n",
    "        (d1_data['run']==run) & (d1_data['daupler_generated']==1)].copy()\n",
    "    d1_ext = d0_data[\n",
    "        (d1_data['run']==run) & (d1_data['daupler_generated']!=1)].copy()\n",
    "    \n",
    "    source_metrics['D0'][run]['dau'] = get_metrics(\n",
    "        data=d0_dau, true_y=\"category\", predicted_y=f\"prediction\", \n",
    "        labels=d0_dau[\"category\"].unique().tolist())\n",
    "    source_metrics['D0'][run]['ext'] = get_metrics(\n",
    "        data=d0_ext, true_y=\"category\", predicted_y=f\"prediction\", \n",
    "        labels=d0_ext[\"category\"].unique().tolist())\n",
    "        \n",
    "    source_metrics['D1'][run]['dau'] = get_metrics(\n",
    "        data=d1_dau, true_y=\"category\", predicted_y=f\"prediction\", \n",
    "        labels=d1_dau[\"category\"].unique().tolist()) \n",
    "    source_metrics['D1'][run]['ext'] = get_metrics(\n",
    "        data=d1_ext, true_y=\"category\", predicted_y=f\"prediction\", \n",
    "        labels=d1_ext[\"category\"].unique().tolist())     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics = pd.DataFrame()\n",
    "for source in ['ext', 'dau']:\n",
    "    d0_f1, d1_f1 = [], []\n",
    "    for run in runs:\n",
    "        d0_f1.append(source_metrics['D0'][run][source].loc[0, 'f1'])\n",
    "        d1_f1.append(source_metrics['D1'][run][source].loc[0, 'f1'])\n",
    "\n",
    "    temp = pd.DataFrame({\n",
    "        \"wghtd_f1\":d0_f1+d1_f1, \n",
    "        \"D\":[f\"D0_{source}\"]*len(runs) + [f\"D1_{source}\"]*len(runs)})\n",
    "    plot_metrics = plot_metrics.append(temp, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(15, 8))\n",
    "f.suptitle(f\"Weighted F1 Performance on Daupler vs External Data\\n(CA-MTL: tinyBERT)\", \n",
    "           fontsize=18)\n",
    "gs = f.add_gridspec(1, 1)\n",
    "f.patch.set_facecolor('white')\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    ax = f.add_subplot(gs[0, 0])\n",
    "    sns.violinplot(x=\"D\", y=\"wghtd_f1\", data=plot_metrics)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Qualifier Task Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/hub/CA-MTL/mock_models/\"\n",
    "runs = {\n",
    "    'stellar-paper-10': ['D0', 'D1'],\n",
    "    'kind-field-12': ['D0', 'D1', 'D2'],\n",
    "    'lilac-dream-14': ['D0', 'D1', 'MANC', 'LOC', 'SIGNT'],\n",
    "    'happy-monkey-15': ['D1', 'MANC', 'LOC', 'SIGNT'],\n",
    "    'dandy-gorge-16': ['D0', 'MANC', 'LOC', 'SIGNT'],\n",
    "    'cool-puddle-17': ['D0', 'D1', 'MANC', 'LOC', 'SIGNT'],\n",
    "    'bright-yogurt-18': ['D0', 'D1'],\n",
    "    }\n",
    "files = os.listdir(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate a dictionary mapping the run files to the appropriate task and fold\n",
    "run_files = defaultdict(defaultdict)\n",
    "for run in runs:\n",
    "    for task in runs[run]:\n",
    "        run_files[run][task] = {}\n",
    "        for partition in ['test', 'dev']:\n",
    "            run_files[run][task][partition] = [\n",
    "                file for file in files if (run in file) & file.startswith(f\"{task}_{partition}\")]\n",
    "# read data            \n",
    "run_data = defaultdict(defaultdict)\n",
    "for run in run_files:\n",
    "    for task in run_files[run]:\n",
    "        dev_path = f\"{model_dir}{run_files[run][task]['dev'][0]}\"\n",
    "        test_path = f\"{model_dir}{run_files[run][task]['test'][0]}\"\n",
    "        data = pd.read_csv(dev_path, sep=\"\\t\")\n",
    "        data['prediction'] = pd.read_csv(test_path, sep=\"\\t\")['prediction']\n",
    "        data['run'] = run\n",
    "        run_data[run][task] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = defaultdict(defaultdict)\n",
    "run_metrics = {}\n",
    "for run in run_data.keys():\n",
    "    run_metrics[run] = pd.DataFrame()\n",
    "    for task in run_data[run].keys():\n",
    "        metrics[run][task] = get_metrics(\n",
    "            data=run_data[run][task], true_y=\"category\", predicted_y=f\"prediction\", \n",
    "            labels=run_data[run][task][\"category\"].unique().tolist())\n",
    "        metrics[run][task]['task'] = task\n",
    "        run_metrics[run] = run_metrics[run].append(metrics[run][task])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_metrics.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get DistilBERT Fold 8 metrics for each task for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a list of text to match for each task in order to make an apples to apples comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_match = {}\n",
    "for task in run_data['lilac-dream-14'].keys():\n",
    "    text_to_match[task] = run_data['lilac-dream-14'][task].text.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbert_dir = \"/hub/311_text_classifier/models\"\n",
    "dbert = {\n",
    "    'D0': f\"{dbert_dir}/D0_development_v1.0_2021_04_08_data_val_preds.csv\",\n",
    "    'D1': f\"{dbert_dir}/D1_development_v1.0_2021_04_08_data_val_preds.csv\",\n",
    "    'LOC': f\"{dbert_dir}/D2_distilbert_locations_prototype_val_preds.csv\",\n",
    "    'MANC': f\"{dbert_dir}/D2_distilbert_manhole_complaint_prototype_val_preds.csv\",\n",
    "    'SIGNT': f\"{dbert_dir}/D2_distilbert_sign_types_prototype_val_preds.csv\",\n",
    "}\n",
    "\n",
    "dbert_data = {}\n",
    "for task in dbert.keys():\n",
    "    # load and clean data to match (unfortunately this new data cleaning was not done originally)\n",
    "    temp = pd.read_csv(dbert[task])\n",
    "    temp['text'] = temp['text'].apply(remove_tabs_newlines)\n",
    "    temp['text'] = temp['text'].apply(remove_multi_spaces)\n",
    "    temp = temp.drop_duplicates('text').reset_index(drop=True)\n",
    "    \n",
    "    # grab only fold 8 text predictions\n",
    "    dbert_data[task] = temp[temp['text'].isin(text_to_match[task])].copy().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get metrics\n",
    "- we are going to be adding these to the metrics dictionaries started above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_cat_pred = {\n",
    "    'D0':{'act': 'D0_category', 'pred': 'D0_predicted_category'},\n",
    "    'D1':{'act': 'D1_category', 'pred': 'D1_predicted_category'},\n",
    "    'LOC':{'act': 'D2_category', 'pred': 'D2_predicted_category'},\n",
    "    'MANC':{'act': 'D2_category', 'pred': 'D2_predicted_category'},\n",
    "    'SIGNT':{'act': 'D2_category', 'pred': 'D2_predicted_category'},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = 'dbert'\n",
    "run_metrics[run] = pd.DataFrame()\n",
    "for task in dbert.keys():\n",
    "    metrics[run][task] = get_metrics(\n",
    "        data=dbert_data[task], true_y=task_cat_pred[task]['act'], \n",
    "        predicted_y=task_cat_pred[task]['pred'], \n",
    "        labels=dbert_data[task][task_cat_pred[task]['act']].unique().tolist())\n",
    "    metrics[run][task]['task'] = task\n",
    "    run_metrics[run] = run_metrics[run].append(metrics[run][task])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_metrics['dbert']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate All Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data_for_merge(data, run, comp_cols, index_col=['task', 'index']):\n",
    "    temp = data[comp_cols].copy()\n",
    "    temp.set_index(index_col, inplace = True)\n",
    "    \n",
    "    run_name = [run]*len(temp.columns)\n",
    "    temp.columns = pd.MultiIndex.from_arrays((run_name, temp.columns))\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "runs = list(run_metrics.keys())\n",
    "comp_cols = ['index','f1', 'task']\n",
    "comparison = prep_data_for_merge(\n",
    "    data=run_metrics[runs[0]], run=runs[0], comp_cols=comp_cols)\n",
    "\n",
    "for run in runs[1:]:\n",
    "    temp =  prep_data_for_merge(\n",
    "        data=run_metrics[run], run=run, comp_cols=comp_cols)\n",
    "    comparison = comparison.merge(temp, how = 'outer', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All metrics from all tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall metrics from all tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison[comparison.index.get_level_values('index').isin(['Overall'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics from specific task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison[comparison.index.get_level_values('task').isin(['D1'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specific label from specific task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'D1'\n",
    "label = 'Water Leaking'\n",
    "pd.DataFrame(comparison.loc[(task, label)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused, deep-dive comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset to relvant tasks, drop any task/labels for which all models are NaN, and compress hierarchical column index for ease of slicing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comp_model_1 = \"bright-yogurt-18\"\n",
    "comp_model_1 = \"dbert\"\n",
    "comp_model_2 = 'cool-puddle-17'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_tasks = [\n",
    "    (comp_model_2, 'f1'),\n",
    "    (comp_model_1, 'f1')]\n",
    "dd_comp = comparison[dd_tasks].copy().dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_comp.columns = [col[0] for col in dd_comp.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find which model did the best for each label in each task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_comp['winner'] = dd_comp.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_overall = dd_comp[dd_comp.index.get_level_values('index').isin(['Overall'])]\n",
    "dd_comp = dd_comp[~dd_comp.index.get_level_values('index').isin(['Overall'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"whitegrid\"):\n",
    "    pd.DataFrame(dd_comp['winner'].value_counts().sort_index()).plot(\n",
    "        kind='bar',\n",
    "        title='All Tasks: Count of labels where model metrics are highest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for task in dd_comp.index.get_level_values('task').unique():\n",
    "    task_comp = dd_comp[dd_comp.index.get_level_values('task').isin([task])]\n",
    "    with sns.axes_style(\"whitegrid\"):\n",
    "        pd.DataFrame(task_comp['winner'].value_counts().sort_index()).plot(\n",
    "            kind='bar',\n",
    "            title=f'{task}: Count of labels where model metrics are highest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much better or worse is DistilBERT doing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_comp['abs_diff'] = np.abs(dd_comp[comp_model_2] - dd_comp[comp_model_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(15, 8))\n",
    "f.suptitle(\n",
    "    f\"All Tasks: Absolute difference between DistilBERT metrics and competitor metrics when each is correct\", \n",
    "    fontsize=18)\n",
    "gs = f.add_gridspec(1, 1)\n",
    "f.patch.set_facecolor('white')\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    ax = f.add_subplot(gs[0, 0])\n",
    "    sns.violinplot(x=\"winner\", y=\"abs_diff\", data=dd_comp)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for task in dd_comp.index.get_level_values('task').unique():\n",
    "    task_comp = dd_comp[dd_comp.index.get_level_values('task').isin([task])].sort_values('winner')\n",
    "    f = plt.figure(figsize=(15, 8))\n",
    "    f.suptitle(f\"{task}: Absolute difference between {comp_model_1} metrics and {comp_model_2} metrics (by winner)\", \n",
    "               fontsize=18)\n",
    "    gs = f.add_gridspec(1, 1)\n",
    "    f.patch.set_facecolor('white')\n",
    "    with sns.axes_style(\"whitegrid\"):\n",
    "        ax = f.add_subplot(gs[0, 0])\n",
    "        sns.violinplot(x=\"winner\", y=\"abs_diff\", data=task_comp)\n",
    "        plt.xticks(fontsize=16)\n",
    "        plt.yticks(fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the top 10 labels that each model is doing better at?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dd_comp[dd_comp['winner'] == comp_model_1].sort_values(\n",
    "    by = 'abs_diff', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_comp[dd_comp['winner'] == comp_model_2].sort_values(\n",
    "    by = 'abs_diff', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### By task comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dd_comp[dd_comp['winner'] == comp_model_1].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dd_comp[dd_comp['winner'] == comp_model_2].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_comp['diff'] = dd_comp[comp_model_1] - dd_comp[comp_model_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for task in dd_comp.index.get_level_values('task').unique():\n",
    "    task_comp = dd_comp[dd_comp.index.get_level_values('task').isin([task])].sort_values(\n",
    "        'winner').copy()\n",
    "    task_comp = task_comp.reset_index(0, drop=True).sort_values(by='diff')\n",
    "    figsize = (15,8)\n",
    "    if task == 'D1':\n",
    "        figsize = (15, 35)\n",
    "    f = plt.figure(figsize=figsize)\n",
    "    f.suptitle(f\"{task}: F1 Metric Difference ({comp_model_1} - {comp_model_2})\\n {comp_model_1}=Blue\",\n",
    "               fontsize=18)\n",
    "    gs = f.add_gridspec(1, 1)\n",
    "    f.patch.set_facecolor('white')\n",
    "    with sns.axes_style(\"whitegrid\"):\n",
    "        ax = f.add_subplot(gs[0, 0])\n",
    "        task_comp['diff'].plot(\n",
    "            kind='barh', color=task_comp.winner.map(\n",
    "                {comp_model_1: 'blue', comp_model_2: 'orange'}))\n",
    "        plt.axvline(x=0, color = 'black')\n",
    "        plt.xticks(fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Comparisons\n",
    "- When the models are wrong, are they wrong in the same way?\n",
    "- When one model is right and the other is wrong, how is it wrong?\n",
    "- When the comparison model is right and the DistilBERT model is wrong, is the DistilBERT model very confident?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_cols = {\n",
    "    'dbert':{\n",
    "        'D0': ['text', 'D0_category', 'D0_predicted_category'],\n",
    "        'D1': ['text', 'D1_category', 'D1_predicted_category'],\n",
    "        'LOC': ['text', 'D2_category', 'D2_predicted_category'],\n",
    "        'SIGNT': ['text', 'D2_category', 'D2_predicted_category'],\n",
    "        'MANC': ['text', 'D2_category', 'D2_predicted_category'],\n",
    "    },\n",
    "    'other':{\n",
    "        'D0': ['text', 'category', 'prediction'],\n",
    "        'D1': ['text', 'category', 'prediction'],\n",
    "        'LOC': ['text', 'category', 'prediction'],\n",
    "        'SIGNT': ['text', 'category', 'prediction'],\n",
    "        'MANC': ['text', 'category', 'prediction'],\n",
    "    },\n",
    "    \n",
    "\n",
    "}\n",
    "cols = ['text', 'category_1', 'prediction_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data = run_data[comp_model_2].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge comparison model and distilbert model data together for direct comparison\n",
    "- Calculate where\n",
    "    - Models disagree\n",
    "    - Models disagree and DistilBERT is wrong\n",
    "    - Models disagree and comparison model is wrong\n",
    "    - Models disagree and both are wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "non_present_tasks = []\n",
    "if comp_model_1 =='dbert':\n",
    "    comp_temp = dbert_data.copy()\n",
    "else:\n",
    "    comp_temp = run_data[comp_model_1].copy()\n",
    "for task in comp_data.keys():\n",
    "    if task in comp_temp.keys():\n",
    "        if comp_model_1 =='dbert':\n",
    "            temp = comp_temp[task][task_cols['dbert'][task]].copy()\n",
    "            temp.columns = cols\n",
    "        else:\n",
    "            temp = comp_temp[task][task_cols['other'][task]].copy()\n",
    "            temp.columns = cols\n",
    "        comp_data[task] = comp_data[task].merge(temp, how = 'left', on = 'text')\n",
    "        comp_data[task]['disagree'] = comp_data[task]['prediction']!=comp_data[task]['prediction_1']\n",
    "        comp_data[task]['wrong_1']=comp_data[task]['category_1']!=comp_data[task]['prediction_1']\n",
    "        comp_data[task]['wrong_2']=comp_data[task]['category']!=comp_data[task]['prediction']\n",
    "        comp_data[task]['disagree_w_1_wrong'] = np.where(\n",
    "            comp_data[task]['disagree'] & comp_data[task]['wrong_1'], True, False)\n",
    "        comp_data[task]['disagree_w_1_wrong_2_right'] = np.where(\n",
    "            comp_data[task]['disagree'] \n",
    "            & comp_data[task]['wrong_1']\n",
    "            & (~comp_data[task]['wrong_2']), True, False)\n",
    "        comp_data[task]['disagree_w_2_wrong'] = np.where(\n",
    "            comp_data[task]['disagree'] & comp_data[task]['wrong_2'], True, False)\n",
    "        comp_data[task]['disagree_w_2_wrong_1_right'] = np.where(\n",
    "            comp_data[task]['disagree'] \n",
    "            & comp_data[task]['wrong_2']\n",
    "            & (~comp_data[task]['wrong_1']), True, False)\n",
    "        comp_data[task]['disagree_w_both_wrong'] = np.where(\n",
    "            comp_data[task]['disagree'] \n",
    "            & comp_data[task]['wrong_1']\n",
    "            & comp_data[task]['wrong_2'], True, False)\n",
    "        comp_data[task]['agree_w_both_wrong'] = np.where(\n",
    "            (~comp_data[task]['disagree'])\n",
    "            & comp_data[task]['wrong_1']\n",
    "            & comp_data[task]['wrong_2'], True, False)\n",
    "        comp_data[task] = comp_data[task].rename(\n",
    "            columns = {'category':'category_2', 'prediction':'prediction_2'})\n",
    "    else:\n",
    "        non_present_tasks += [task]\n",
    "for task in non_present_tasks:\n",
    "    comp_data.pop(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How often do the two models disagree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for task in comp_data.keys():\n",
    "    with sns.axes_style(\"whitegrid\"):\n",
    "        pd.DataFrame(\n",
    "            comp_data[task]['disagree'].value_counts()/comp_data[task].shape[0]).plot(\n",
    "            kind='barh', title = f\"{task}: Model Agreement Percentages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How often do they disagree but comp_model_1 is right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for task in comp_data.keys():\n",
    "    with sns.axes_style(\"whitegrid\"):\n",
    "        pd.DataFrame(\n",
    "            comp_data[task]['disagree_w_2_wrong_1_right'].value_counts()/comp_data[task].shape[0]).plot(\n",
    "            kind='barh', title = f\"{task}: Model Disagreement Percentages, {comp_model_1} right, {comp_model_2} wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How often do they disagree but comp_model_2 is right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for task in comp_data.keys():\n",
    "    with sns.axes_style(\"whitegrid\"):\n",
    "        pd.DataFrame(\n",
    "            comp_data[task]['disagree_w_1_wrong_2_right'].value_counts()/comp_data[task].shape[0]).plot(\n",
    "            kind='barh', title = f\"{task}: Model Disagreement Percentages, {comp_model_1} wrong, {comp_model_2} right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How often do they disagree when they are both wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for task in comp_data.keys():\n",
    "    with sns.axes_style(\"whitegrid\"):\n",
    "        pd.DataFrame(\n",
    "            comp_data[task]['disagree_w_both_wrong'].value_counts()/comp_data[task].shape[0]).plot(\n",
    "            kind='barh', title = f\"{task}: Model Agreement Percentages when both are wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in comp_data.keys():\n",
    "    print(f\"\\nBOTH MODELS ARE WRONG AND DISAGREE\")\n",
    "    print(\"+\"*40 + \" \" + task + \" \" + \"+\"*40)\n",
    "    temp = comp_data[task][\n",
    "        comp_data[task]['disagree_w_both_wrong']].copy().reset_index(drop=True).sort_values('category_2')\n",
    "    display(temp['category_1'].value_counts())\n",
    "    for i in range(temp.shape[0]):\n",
    "        record = temp.iloc[i]\n",
    "        print(\"=\"*80)\n",
    "        print(record['text'])\n",
    "        print(f\"Category: {record['category_2']}\")\n",
    "        print(f\"Prediction: {comp_model_1} = {record['prediction_1']}; {comp_model_2} = {record['prediction_2']}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where is model 1 right but the model 2 wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task = 'D1'\n",
    "right_1 = (~comp_data[task]['wrong_1'])\n",
    "wrong_2 = comp_data[task]['wrong_2']\n",
    "wrong_1 = comp_data[task]['wrong_1']\n",
    "right_2 = (~comp_data[task]['wrong_2'])\n",
    "right_1_wrong_2 = comp_data[task][right_1 & wrong_2]\n",
    "wrong_1_right_2 = comp_data[task][wrong_1 & right_2]\n",
    "\n",
    "print(f\"Count of {task} where {comp_model_1} is right and {comp_model_2} is wrong {right_1_wrong_2.shape[0]}\")\n",
    "print(f\"Count of {task} where {comp_model_1} is wrong and {comp_model_2} is right {wrong_1_right_2.shape[0]}\")\n",
    "\n",
    "wrong_comp = pd.DataFrame(\n",
    "    wrong_1_right_2['category_1'].value_counts()).rename(columns={'category_1':'wrong_1_right_2'})\n",
    "right_comp = pd.DataFrame(\n",
    "    right_1_wrong_2['category_1'].value_counts()).rename(columns={'category_1':'right_1_wrong_2'})\n",
    "right_wrong_comp = wrong_comp.merge(right_comp, how = 'outer', \n",
    "                                    left_index=True, right_index=True).fillna(0).astype(int)\n",
    "right_wrong_comp.loc[\"Total\"] = right_wrong_comp.sum()\n",
    "right_wrong_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'Traffic Study'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = right_1_wrong_2[right_1_wrong_2['category_1']==category]\n",
    "for i in range(temp.shape[0]):\n",
    "    record = temp.iloc[i]\n",
    "    print(\"=\"*80)\n",
    "    print(record['text'])\n",
    "    print(f\"Category: {record['category_1']} ({record['category_2']})\")\n",
    "    print(f\"Prediction: {comp_model_2} = {record['prediction_2']}; {comp_model_1} = {record['prediction_1']}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = wrong_1_right_2[wrong_1_right_2['category_1']==category]\n",
    "for i in range(temp.shape[0]):\n",
    "    record = temp.iloc[i]\n",
    "    print(\"=\"*80)\n",
    "    print(record['text'])\n",
    "    print(f\"Category: {record['category_1']} ({record['category_2']})\")\n",
    "    print(f\"Prediction: {comp_model_2} = {record['prediction_2']}; {comp_model_1} = {record['prediction_1']}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The caller forgot to pay her water bill and she is stating that she will come in tomorrow to pay it in full.\"\n",
    "comp_data['D1'][comp_data['D1'].text==text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python run.py \\\n",
    "# --model_name_or_path CA-MTL-tiny \\\n",
    "# --data_dir /hub/CA-MTL/data \\\n",
    "# --output_dir /hub/CA-MTL/mock_models \\\n",
    "# --tasks D0 D1 MANC LOC SIGNT \\\n",
    "# --overwrite_cache \\\n",
    "# --task_data_folders D0/2021_04_08 D1/2021_04_08 MANC/2021_04_08 LOC/2021_04_08 SIGNT/2021_04_08 \\\n",
    "# --do_train \\\n",
    "# --do_eval \\\n",
    "# --do_predict \\\n",
    "# --evaluate_during_training \\\n",
    "# --per_device_train_batch_size 32 \\\n",
    "# --per_device_eval_batch_size 32 \\\n",
    "# --learning_rate 5e-5 \\\n",
    "# --adam_epsilon 1e-8 \\\n",
    "# --num_train_epochs 7 \\\n",
    "# --warmup_steps 0 \\\n",
    "# --save_steps 1500 \\\n",
    "# --save_total_limit 1 \\\n",
    "# --seed 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[file for file in os.listdir('/hub/CA-MTL/mock_models/') if 'royal-lion' in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('/hub/CA-MTL/mock_models/D0_test_iter_royal-lion-29.tsv', sep='\\t')\n",
    "logits = test['logits'].apply(lambda x: pd.Series(eval(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ca-mtl-env",
   "language": "python",
   "name": "ca-mtl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
