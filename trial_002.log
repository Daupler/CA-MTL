04/12/2021 07:09:47 - INFO - transformers.training_args -   PyTorch: setting up devices
04/12/2021 07:09:47 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
04/12/2021 07:09:47 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/huawei-noah/TinyBERT_General_6L_768D/config.json from cache at /home/datasci/.cache/torch/transformers/a2effe717003a3a536ff7bebc6107c97073ca65e9878930cb8b130651bb43b0f.1173f0740b7f056b864067e238c1c50ae7bb3344d34377e77818b5d00e944279
04/12/2021 07:09:47 - INFO - transformers.configuration_utils -   Model config BertConfig {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pad_token_id": 0,
  "pre_trained": "",
  "structure": [],
  "type_vocab_size": 2,
  "vocab_size": 30522
}

04/12/2021 07:09:47 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/huawei-noah/TinyBERT_General_6L_768D/pytorch_model.bin from cache at /home/datasci/.cache/torch/transformers/84451f9577b29a4f236f28261fd2603640828cb26a7fed13f70d043ddbca41d5.9d208358bccc05e7a1e0957ce9f4ad622ec4eeeadc384905fd19b6763c9cec2f
04/12/2021 07:09:49 - INFO - transformers.modeling_utils -   Weights of CaMtl not initialized from pretrained model: ['bert.task_type_embeddings.weight', 'bert.conditional_alignment.gb_weights.weight', 'bert.conditional_alignment.gb_weights.bias', 'bert.encoder.task_transformation.weight', 'bert.encoder.task_transformation.bias', 'bert.encoder.layer.0.attention.self.random_weight_matrix', 'bert.encoder.layer.0.attention.self.cond_block_diag_attn.gb_weights.weight', 'bert.encoder.layer.0.attention.self.cond_block_diag_attn.gb_weights.bias', 'bert.encoder.layer.1.attention.self.random_weight_matrix', 'bert.encoder.layer.1.attention.self.cond_block_diag_attn.gb_weights.weight', 'bert.encoder.layer.1.attention.self.cond_block_diag_attn.gb_weights.bias', 'bert.encoder.layer.2.attention.self.random_weight_matrix', 'bert.encoder.layer.2.attention.self.cond_block_diag_attn.gb_weights.weight', 'bert.encoder.layer.2.attention.self.cond_block_diag_attn.gb_weights.bias', 'bert.encoder.layer.3.attention.self.random_weight_matrix', 'bert.encoder.layer.3.attention.self.cond_block_diag_attn.gb_weights.weight', 'bert.encoder.layer.3.attention.self.cond_block_diag_attn.gb_weights.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.ln_weight_modulation.gb_weights.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.ln_weight_modulation.gb_weights.bias', 'bert.encoder.layer.3.output.LayerNorm.ln_weight_modulation.gb_weights.weight', 'bert.encoder.layer.3.output.LayerNorm.ln_weight_modulation.gb_weights.bias', 'bert.encoder.layer.4.attention.self.random_weight_matrix', 'bert.encoder.layer.4.attention.self.cond_block_diag_attn.gb_weights.weight', 'bert.encoder.layer.4.attention.self.cond_block_diag_attn.gb_weights.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.ln_weight_modulation.gb_weights.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.ln_weight_modulation.gb_weights.bias', 'bert.encoder.layer.4.output.LayerNorm.ln_weight_modulation.gb_weights.weight', 'bert.encoder.layer.4.output.LayerNorm.ln_weight_modulation.gb_weights.bias', 'bert.encoder.layer.5.new_attention.self.random_weight_matrix', 'bert.encoder.layer.5.new_attention.self.query.weight', 'bert.encoder.layer.5.new_attention.self.query.bias', 'bert.encoder.layer.5.new_attention.self.key.weight', 'bert.encoder.layer.5.new_attention.self.key.bias', 'bert.encoder.layer.5.new_attention.self.value.weight', 'bert.encoder.layer.5.new_attention.self.value.bias', 'bert.encoder.layer.5.new_attention.self.cond_block_diag_attn.gb_weights.weight', 'bert.encoder.layer.5.new_attention.self.cond_block_diag_attn.gb_weights.bias', 'bert.encoder.layer.5.new_attention.output.dense.weight', 'bert.encoder.layer.5.new_attention.output.dense.bias', 'bert.encoder.layer.5.new_attention.output.LayerNorm.weight', 'bert.encoder.layer.5.new_attention.output.LayerNorm.bias', 'bert.encoder.layer.5.new_attention.output.LayerNorm.ln_weight_modulation.gb_weights.weight', 'bert.encoder.layer.5.new_attention.output.LayerNorm.ln_weight_modulation.gb_weights.bias', 'bert.encoder.layer.5.new_intermediate.dense.weight', 'bert.encoder.layer.5.new_intermediate.dense.bias', 'bert.encoder.layer.5.new_output.dense.weight', 'bert.encoder.layer.5.new_output.dense.bias', 'bert.encoder.layer.5.new_output.LayerNorm.weight', 'bert.encoder.layer.5.new_output.LayerNorm.bias', 'bert.encoder.layer.5.new_output.LayerNorm.ln_weight_modulation.gb_weights.weight', 'bert.encoder.layer.5.new_output.LayerNorm.ln_weight_modulation.gb_weights.bias', 'bert.encoder.layer.5.adapter.bottleneck.emb_transf.weight', 'bert.encoder.layer.5.adapter.bottleneck.emb_transf.bias', 'bert.encoder.layer.5.adapter.bottleneck.hidden_modulation.gb_weights.weight', 'bert.encoder.layer.5.adapter.bottleneck.hidden_modulation.gb_weights.bias', 'bert.encoder.layer.5.adapter.bottleneck.down_proj_layer.weight', 'bert.encoder.layer.5.adapter.bottleneck.down_proj_layer.bias', 'bert.encoder.layer.5.adapter.bottleneck.up_proj_layer.weight', 'bert.encoder.layer.5.adapter.bottleneck.up_proj_layer.bias', 'bert.encoder.layer.5.adapter.condlayernorm.weight', 'bert.encoder.layer.5.adapter.condlayernorm.bias', 'bert.encoder.layer.5.adapter.condlayernorm.ln_weight_modulation.gb_weights.weight', 'bert.encoder.layer.5.adapter.condlayernorm.ln_weight_modulation.gb_weights.bias', 'decoders.0.model.weight', 'decoders.0.model.bias', 'decoders.1.model.weight', 'decoders.1.model.bias']
04/12/2021 07:09:49 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in CaMtl: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.0.weight', 'fit_denses.0.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.2.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.4.bias', 'fit_denses.5.weight', 'fit_denses.5.bias', 'fit_denses.6.weight', 'fit_denses.6.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias']
04/12/2021 07:09:49 - INFO - __main__ -   CaMtl(
  (bert): CaMtlBaseEncoder(
    (task_type_embeddings): Embedding(2, 768)
    (conditional_alignment): FiLM(
      (gb_weights): Linear(in_features=768, out_features=1536, bias=True)
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): MyBertEncoder9(
      (task_transformation): Linear(in_features=768, out_features=768, bias=True)
      (layer): ModuleList(
        (0): BertLayer9(
          (attention): MyBertAttention9(
            (self): MyBertSelfAttention9(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (cond_block_diag_attn): CBDA(
                (gb_weights): Linear(in_features=768, out_features=172, bias=True)
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer9(
          (attention): MyBertAttention9(
            (self): MyBertSelfAttention9(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (cond_block_diag_attn): CBDA(
                (gb_weights): Linear(in_features=768, out_features=172, bias=True)
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer9(
          (attention): MyBertAttention9(
            (self): MyBertSelfAttention9(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (cond_block_diag_attn): CBDA(
                (gb_weights): Linear(in_features=768, out_features=172, bias=True)
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): MyBertLayer9(
          (attention): MyBertAttention9(
            (self): MyBertSelfAttention9(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (cond_block_diag_attn): CBDA(
                (gb_weights): Linear(in_features=768, out_features=172, bias=True)
              )
            )
            (output): MyBertSelfOutput9(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): ConditionalLayerNorm(
                (768,), 768, eps=1e-12
                (ln_weight_modulation): FiLM(
                  (gb_weights): Linear(in_features=768, out_features=1536, bias=True)
                )
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): MyBertOutput9(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): ConditionalLayerNorm(
              (768,), 768, eps=1e-12
              (ln_weight_modulation): FiLM(
                (gb_weights): Linear(in_features=768, out_features=1536, bias=True)
              )
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): MyBertLayer9(
          (attention): MyBertAttention9(
            (self): MyBertSelfAttention9(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (cond_block_diag_attn): CBDA(
                (gb_weights): Linear(in_features=768, out_features=172, bias=True)
              )
            )
            (output): MyBertSelfOutput9(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): ConditionalLayerNorm(
                (768,), 768, eps=1e-12
                (ln_weight_modulation): FiLM(
                  (gb_weights): Linear(in_features=768, out_features=1536, bias=True)
                )
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): MyBertOutput9(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): ConditionalLayerNorm(
              (768,), 768, eps=1e-12
              (ln_weight_modulation): FiLM(
                (gb_weights): Linear(in_features=768, out_features=1536, bias=True)
              )
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): MyBertAdapterLayer9(
          (new_attention): MyBertAttention9(
            (self): MyBertSelfAttention9(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (cond_block_diag_attn): CBDA(
                (gb_weights): Linear(in_features=768, out_features=172, bias=True)
              )
            )
            (output): MyBertSelfOutput9(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): ConditionalLayerNorm(
                (768,), 768, eps=1e-12
                (ln_weight_modulation): FiLM(
                  (gb_weights): Linear(in_features=768, out_features=1536, bias=True)
                )
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (new_intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (new_output): MyBertOutput9(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): ConditionalLayerNorm(
              (768,), 768, eps=1e-12
              (ln_weight_modulation): FiLM(
                (gb_weights): Linear(in_features=768, out_features=1536, bias=True)
              )
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (adapter): BertAdapter9(
            (bottleneck): ConditionalBottleNeck(
              (emb_transf): Linear(in_features=768, out_features=768, bias=True)
              (hidden_modulation): FiLM(
                (gb_weights): Linear(in_features=768, out_features=1536, bias=True)
              )
              (down_proj_layer): Linear(in_features=768, out_features=256, bias=True)
              (up_proj_layer): Linear(in_features=256, out_features=768, bias=True)
            )
            (condlayernorm): ConditionalLayerNorm(
              (768,), 768, eps=1e-12
              (ln_weight_modulation): FiLM(
                (gb_weights): Linear(in_features=768, out_features=1536, bias=True)
              )
            )
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (decoders): ModuleList(
    (0): Decoder(
      (dropout): Dropout(p=0.1, inplace=False)
      (model): Linear(in_features=768, out_features=17, bias=True)
    )
    (1): Decoder(
      (dropout): Dropout(p=0.1, inplace=False)
      (model): Linear(in_features=768, out_features=114, bias=True)
    )
  )
)
04/12/2021 07:09:49 - INFO - transformers.tokenization_utils -   Model name 'huawei-noah/TinyBERT_General_6L_768D' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'huawei-noah/TinyBERT_General_6L_768D' is a path, a model identifier, or url to a directory containing tokenizer files.
04/12/2021 07:09:51 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/huawei-noah/TinyBERT_General_6L_768D/vocab.txt from cache at /home/datasci/.cache/torch/transformers/f3df129df0d2a6b551ca9c604c897a2fffe0138d43f74685fa0fe4837e51f986.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
04/12/2021 07:09:51 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/huawei-noah/TinyBERT_General_6L_768D/added_tokens.json from cache at None
04/12/2021 07:09:51 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/huawei-noah/TinyBERT_General_6L_768D/special_tokens_map.json from cache at None
04/12/2021 07:09:51 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/huawei-noah/TinyBERT_General_6L_768D/tokenizer_config.json from cache at None
04/12/2021 07:09:51 - INFO - __main__ -   Training tasks: D0, D1
04/12/2021 07:09:51 - INFO - filelock -   Lock 139910707853712 acquired on /hub/CA-MTL/data/D0/2021_04_08/cached_train_BertTokenizer_256_D0.lock
04/12/2021 07:09:51 - INFO - src.data.data_utils -   Loading features from cached file /hub/CA-MTL/data/D0/2021_04_08/cached_train_BertTokenizer_256_D0 [took 0.001 s]
04/12/2021 07:09:51 - INFO - filelock -   Lock 139910707853712 released on /hub/CA-MTL/data/D0/2021_04_08/cached_train_BertTokenizer_256_D0.lock
04/12/2021 07:09:51 - INFO - filelock -   Lock 139910707854544 acquired on /hub/CA-MTL/data/D1/2021_04_08/cached_train_BertTokenizer_256_D1.lock
04/12/2021 07:09:51 - INFO - src.data.data_utils -   Loading features from cached file /hub/CA-MTL/data/D1/2021_04_08/cached_train_BertTokenizer_256_D1 [took 0.000 s]
04/12/2021 07:09:51 - INFO - filelock -   Lock 139910707854544 released on /hub/CA-MTL/data/D1/2021_04_08/cached_train_BertTokenizer_256_D1.lock
04/12/2021 07:09:51 - INFO - filelock -   Lock 139910707852112 acquired on /hub/CA-MTL/data/D0/2021_04_08/cached_dev_BertTokenizer_256_D0.lock
04/12/2021 07:09:51 - INFO - src.data.data_utils -   Loading features from cached file /hub/CA-MTL/data/D0/2021_04_08/cached_dev_BertTokenizer_256_D0 [took 0.026 s]
04/12/2021 07:09:51 - INFO - filelock -   Lock 139910707852112 released on /hub/CA-MTL/data/D0/2021_04_08/cached_dev_BertTokenizer_256_D0.lock
04/12/2021 07:09:51 - INFO - filelock -   Lock 139910707854736 acquired on /hub/CA-MTL/data/D1/2021_04_08/cached_dev_BertTokenizer_256_D1.lock
04/12/2021 07:09:51 - INFO - src.data.data_utils -   Loading features from cached file /hub/CA-MTL/data/D1/2021_04_08/cached_dev_BertTokenizer_256_D1 [took 0.025 s]
04/12/2021 07:09:51 - INFO - filelock -   Lock 139910707854736 released on /hub/CA-MTL/data/D1/2021_04_08/cached_dev_BertTokenizer_256_D1.lock
04/12/2021 07:09:51 - INFO - filelock -   Lock 139910707854544 acquired on /hub/CA-MTL/data/D0/2021_04_08/cached_test_BertTokenizer_256_D0.lock
04/12/2021 07:09:51 - INFO - src.data.data_utils -   Loading features from cached file /hub/CA-MTL/data/D0/2021_04_08/cached_test_BertTokenizer_256_D0 [took 0.247 s]
04/12/2021 07:09:51 - INFO - filelock -   Lock 139910707854544 released on /hub/CA-MTL/data/D0/2021_04_08/cached_test_BertTokenizer_256_D0.lock
04/12/2021 07:09:51 - INFO - filelock -   Lock 139910162899344 acquired on /hub/CA-MTL/data/D1/2021_04_08/cached_test_BertTokenizer_256_D1.lock
04/12/2021 07:09:51 - INFO - src.data.data_utils -   Loading features from cached file /hub/CA-MTL/data/D1/2021_04_08/cached_test_BertTokenizer_256_D1 [took 0.178 s]
04/12/2021 07:09:51 - INFO - filelock -   Lock 139910162899344 released on /hub/CA-MTL/data/D1/2021_04_08/cached_test_BertTokenizer_256_D1.lock
04/12/2021 07:09:53 - WARNING - transformers.trainer -   You are instantiating a Trainer but Tensorboard is not installed. You should consider installing it.
04/12/2021 07:09:53 - INFO - transformers.trainer -   Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: heatxg (use `wandb login --relogin` to force relogin)
wandb: Tracking run with wandb version 0.10.25
wandb: Syncing run solar-night-10
wandb: ‚≠êÔ∏è View project at https://wandb.ai/heatxg/huggingface
wandb: üöÄ View run at https://wandb.ai/heatxg/huggingface/runs/2nzy21qj
wandb: Run data is saved locally in /home/datasci/CA-MTL/wandb/run-20210412_070953-2nzy21qj
wandb: Run `wandb offline` to turn off syncing.
04/12/2021 07:09:54 - INFO - transformers.trainer -   ***** Running training *****
04/12/2021 07:09:54 - INFO - transformers.trainer -     Num examples = 100
04/12/2021 07:09:54 - INFO - transformers.trainer -     Num Epochs = 7
04/12/2021 07:09:54 - INFO - transformers.trainer -     Instantaneous batch size per device = 32
04/12/2021 07:09:54 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 32
04/12/2021 07:09:54 - INFO - transformers.trainer -     Gradient Accumulation steps = 1
04/12/2021 07:09:54 - INFO - transformers.trainer -     Total optimization steps = 28
Epoch:   0%|                                              | 0/7 [00:00<?, ?it/s]
Iteration:   0%|                                          | 0/4 [00:00<?, ?it/s][A
Iteration:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 1/4 [00:00<00:01,  1.75it/s][A
Iteration:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 2/4 [00:00<00:01,  1.93it/s][A
Iteration:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 3/4 [00:01<00:00,  2.08it/s][AIteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.77it/s]
Epoch:  14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                | 1/7 [00:01<00:08,  1.45s/it]
Iteration:   0%|                                          | 0/4 [00:00<?, ?it/s][A
Iteration:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 1/4 [00:00<00:01,  2.53it/s][A
Iteration:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 2/4 [00:00<00:00,  2.51it/s][A
Iteration:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 3/4 [00:01<00:00,  2.50it/s][AIteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  3.11it/s]
Epoch:  29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 2/7 [00:02<00:06,  1.40s/it]
Iteration:   0%|                                          | 0/4 [00:00<?, ?it/s][A
Iteration:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 1/4 [00:00<00:01,  2.50it/s][A
Iteration:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 2/4 [00:00<00:00,  2.51it/s][A
Iteration:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 3/4 [00:01<00:00,  2.50it/s][AIteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  3.08it/s]
Epoch:  43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 3/7 [00:04<00:05,  1.37s/it]
Iteration:   0%|                                          | 0/4 [00:00<?, ?it/s][A
Iteration:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 1/4 [00:00<00:01,  2.51it/s][A
Iteration:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 2/4 [00:00<00:00,  2.51it/s][A
Iteration:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 3/4 [00:01<00:00,  2.52it/s][AIteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  3.11it/s]
Epoch:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 4/7 [00:05<00:04,  1.34s/it]
Iteration:   0%|                                          | 0/4 [00:00<?, ?it/s][A
Iteration:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 1/4 [00:00<00:01,  2.54it/s][A
Iteration:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 2/4 [00:00<00:00,  2.52it/s][A
Iteration:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 3/4 [00:01<00:00,  2.51it/s][AIteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  3.10it/s]
Epoch:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 5/7 [00:06<00:02,  1.33s/it]
Iteration:   0%|                                          | 0/4 [00:00<?, ?it/s][A
Iteration:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 1/4 [00:00<00:01,  2.52it/s][A
Iteration:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 2/4 [00:00<00:00,  2.52it/s][A
Iteration:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 3/4 [00:01<00:00,  2.51it/s][AIteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  3.10it/s]
Epoch:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 6/7 [00:07<00:01,  1.32s/it]
Iteration:   0%|                                          | 0/4 [00:00<?, ?it/s][A
Iteration:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 1/4 [00:00<00:01,  2.51it/s][A
Iteration:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 2/4 [00:00<00:00,  2.51it/s][A
Iteration:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 3/4 [00:01<00:00,  2.52it/s][AIteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  3.11it/s]
Epoch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:09<00:00,  1.31s/it]Epoch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:09<00:00,  1.31s/it]
04/12/2021 07:10:03 - INFO - transformers.trainer -   

Training completed. Do not forget to share your model on huggingface.co/models =)


04/12/2021 07:10:03 - INFO - src.mtl_trainer -   *** Evaluate on dev ***
04/12/2021 07:10:03 - INFO - src.mtl_trainer -   D0
04/12/2021 07:10:03 - INFO - transformers.trainer -   ***** Running Evaluation *****
04/12/2021 07:10:03 - INFO - transformers.trainer -     Num examples = 1776
04/12/2021 07:10:03 - INFO - transformers.trainer -     Batch size = 32
Evaluation:   0%|                                        | 0/56 [00:00<?, ?it/s]Evaluation:   2%|‚ñå                               | 1/56 [00:00<00:07,  7.62it/s]Evaluation:   4%|‚ñà‚ñè                              | 2/56 [00:00<00:07,  7.63it/s]Evaluation:   5%|‚ñà‚ñã                              | 3/56 [00:00<00:06,  7.63it/s]Evaluation:   7%|‚ñà‚ñà‚ñé                             | 4/56 [00:00<00:06,  7.62it/s]Evaluation:   9%|‚ñà‚ñà‚ñä                             | 5/56 [00:00<00:06,  7.62it/s]Evaluation:  11%|‚ñà‚ñà‚ñà‚ñç                            | 6/56 [00:00<00:06,  7.62it/s]Evaluation:  12%|‚ñà‚ñà‚ñà‚ñà                            | 7/56 [00:00<00:06,  7.62it/s]Evaluation:  14%|‚ñà‚ñà‚ñà‚ñà‚ñå                           | 8/56 [00:01<00:06,  7.62it/s]Evaluation:  16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 9/56 [00:01<00:06,  7.62it/s]Evaluation:  18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 10/56 [00:01<00:06,  7.61it/s]Evaluation:  20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 11/56 [00:01<00:05,  7.62it/s]Evaluation:  21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 12/56 [00:01<00:05,  7.62it/s]Evaluation:  23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                       | 13/56 [00:01<00:05,  7.60it/s]Evaluation:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 14/56 [00:01<00:05,  7.60it/s]Evaluation:  27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 15/56 [00:01<00:05,  7.63it/s]Evaluation:  29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 16/56 [00:02<00:05,  7.63it/s]Evaluation:  30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 17/56 [00:02<00:05,  7.60it/s]Evaluation:  32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 18/56 [00:02<00:04,  7.62it/s]Evaluation:  34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 19/56 [00:02<00:04,  7.61it/s]Evaluation:  36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 20/56 [00:02<00:04,  7.62it/s]Evaluation:  38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 21/56 [00:02<00:04,  7.63it/s]Evaluation:  39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 22/56 [00:02<00:04,  7.64it/s]Evaluation:  41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 23/56 [00:03<00:04,  7.63it/s]Evaluation:  43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 24/56 [00:03<00:04,  7.63it/s]Evaluation:  45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 25/56 [00:03<00:04,  7.64it/s]Evaluation:  46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 26/56 [00:03<00:03,  7.64it/s]Evaluation:  48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 27/56 [00:03<00:03,  7.68it/s]Evaluation:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 28/56 [00:03<00:03,  7.66it/s]Evaluation:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 29/56 [00:03<00:03,  7.66it/s]Evaluation:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 30/56 [00:03<00:03,  7.65it/s]Evaluation:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 31/56 [00:04<00:03,  7.62it/s]Evaluation:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 32/56 [00:04<00:03,  7.63it/s]Evaluation:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 33/56 [00:04<00:03,  7.66it/s]Evaluation:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 34/56 [00:04<00:02,  7.65it/s]Evaluation:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 35/56 [00:04<00:02,  7.62it/s]Evaluation:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 36/56 [00:04<00:02,  7.62it/s]Evaluation:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 37/56 [00:04<00:02,  7.61it/s]Evaluation:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 38/56 [00:04<00:02,  7.61it/s]Evaluation:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 39/56 [00:05<00:02,  7.61it/s]Evaluation:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 40/56 [00:05<00:02,  7.61it/s]Evaluation:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 41/56 [00:05<00:01,  7.61it/s]Evaluation:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 42/56 [00:05<00:01,  7.61it/s]Evaluation:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 43/56 [00:05<00:01,  7.63it/s]Evaluation:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 44/56 [00:05<00:01,  7.62it/s]Evaluation:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 45/56 [00:05<00:01,  7.61it/s]Evaluation:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 46/56 [00:06<00:01,  7.61it/s]Evaluation:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 47/56 [00:06<00:01,  7.61it/s]Evaluation:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 48/56 [00:06<00:01,  7.62it/s]Evaluation:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 49/56 [00:06<00:00,  7.62it/s]Evaluation:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 50/56 [00:06<00:00,  7.61it/s]Evaluation:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 51/56 [00:06<00:00,  7.62it/s]Evaluation:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 52/56 [00:06<00:00,  7.66it/s]Evaluation:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 53/56 [00:06<00:00,  7.63it/s]Evaluation:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 54/56 [00:07<00:00,  7.63it/s]Evaluation:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 55/56 [00:07<00:00,  7.62it/s]Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 56/56 [00:07<00:00,  7.69it/s]
/home/datasci/anaconda3/envs/ca-mtl-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
04/12/2021 07:10:10 - INFO - src.mtl_trainer -     eval_loss = 3.667119571140834
04/12/2021 07:10:10 - INFO - src.mtl_trainer -     eval_matthews_corrcoef = 0.0
04/12/2021 07:10:10 - INFO - src.mtl_trainer -     eval_acc = 0.1266891891891892
04/12/2021 07:10:10 - INFO - src.mtl_trainer -     eval_f1_micro = 0.1266891891891892
04/12/2021 07:10:10 - INFO - src.mtl_trainer -     eval_f1_weighted = 0.02849082215648932
04/12/2021 07:10:10 - INFO - src.mtl_trainer -     eval_acc_and_f1_weighted = 0.07759000567283926
04/12/2021 07:10:10 - INFO - src.mtl_trainer -     epoch = 7.0
04/12/2021 07:10:10 - INFO - src.mtl_trainer -   D1
04/12/2021 07:10:10 - INFO - transformers.trainer -   ***** Running Evaluation *****
04/12/2021 07:10:10 - INFO - transformers.trainer -     Num examples = 1813
04/12/2021 07:10:10 - INFO - transformers.trainer -     Batch size = 32
Evaluation:   0%|                                        | 0/57 [00:00<?, ?it/s]Evaluation:   2%|‚ñå                               | 1/57 [00:00<00:07,  7.66it/s]Evaluation:   4%|‚ñà                               | 2/57 [00:00<00:07,  7.65it/s]Evaluation:   5%|‚ñà‚ñã                              | 3/57 [00:00<00:07,  7.64it/s]Evaluation:   7%|‚ñà‚ñà‚ñè                             | 4/57 [00:00<00:06,  7.64it/s]Evaluation:   9%|‚ñà‚ñà‚ñä                             | 5/57 [00:00<00:06,  7.63it/s]Evaluation:  11%|‚ñà‚ñà‚ñà‚ñé                            | 6/57 [00:00<00:06,  7.62it/s]Evaluation:  12%|‚ñà‚ñà‚ñà‚ñâ                            | 7/57 [00:00<00:06,  7.62it/s]Evaluation:  14%|‚ñà‚ñà‚ñà‚ñà‚ñç                           | 8/57 [00:01<00:06,  7.62it/s]Evaluation:  16%|‚ñà‚ñà‚ñà‚ñà‚ñà                           | 9/57 [00:01<00:06,  7.62it/s]Evaluation:  18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 10/57 [00:01<00:06,  7.61it/s]Evaluation:  19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 11/57 [00:01<00:06,  7.61it/s]Evaluation:  21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 12/57 [00:01<00:05,  7.64it/s]Evaluation:  23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 13/57 [00:01<00:05,  7.63it/s]Evaluation:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 14/57 [00:01<00:05,  7.60it/s]Evaluation:  26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                      | 15/57 [00:01<00:05,  7.62it/s]Evaluation:  28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                      | 16/57 [00:02<00:05,  7.63it/s]Evaluation:  30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                     | 17/57 [00:02<00:05,  7.63it/s]Evaluation:  32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                     | 18/57 [00:02<00:05,  7.64it/s]Evaluation:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 19/57 [00:02<00:04,  7.63it/s]Evaluation:  35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 20/57 [00:02<00:04,  7.64it/s]Evaluation:  37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 21/57 [00:02<00:04,  7.64it/s]Evaluation:  39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 22/57 [00:02<00:04,  7.63it/s]Evaluation:  40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 23/57 [00:03<00:04,  7.62it/s]Evaluation:  42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 24/57 [00:03<00:04,  7.61it/s]Evaluation:  44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                 | 25/57 [00:03<00:04,  7.61it/s]Evaluation:  46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 26/57 [00:03<00:04,  7.61it/s]Evaluation:  47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 27/57 [00:03<00:03,  7.65it/s]Evaluation:  49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 28/57 [00:03<00:03,  7.63it/s]Evaluation:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 29/57 [00:03<00:03,  7.63it/s]Evaluation:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 30/57 [00:03<00:03,  7.65it/s]Evaluation:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 31/57 [00:04<00:03,  7.63it/s]Evaluation:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 32/57 [00:04<00:03,  7.60it/s]Evaluation:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 33/57 [00:04<00:03,  7.62it/s]Evaluation:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 34/57 [00:04<00:02,  7.69it/s]Evaluation:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 35/57 [00:04<00:02,  7.66it/s]Evaluation:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 36/57 [00:04<00:02,  7.63it/s]Evaluation:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 37/57 [00:04<00:02,  7.62it/s]Evaluation:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã          | 38/57 [00:04<00:02,  7.62it/s]Evaluation:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 39/57 [00:05<00:02,  7.63it/s]Evaluation:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 40/57 [00:05<00:02,  7.64it/s]Evaluation:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 41/57 [00:05<00:02,  7.67it/s]Evaluation:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 42/57 [00:05<00:01,  7.63it/s]Evaluation:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 43/57 [00:05<00:01,  7.63it/s]Evaluation:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 44/57 [00:05<00:01,  7.63it/s]Evaluation:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 45/57 [00:05<00:01,  7.64it/s]Evaluation:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 46/57 [00:06<00:01,  7.62it/s]Evaluation:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 47/57 [00:06<00:01,  7.62it/s]Evaluation:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 48/57 [00:06<00:01,  7.65it/s]Evaluation:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 49/57 [00:06<00:01,  7.63it/s]Evaluation:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 50/57 [00:06<00:00,  7.61it/s]Evaluation:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 51/57 [00:06<00:00,  7.61it/s]Evaluation:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 52/57 [00:06<00:00,  7.62it/s]Evaluation:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 53/57 [00:06<00:00,  7.62it/s]Evaluation:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 54/57 [00:07<00:00,  7.62it/s]Evaluation:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 55/57 [00:07<00:00,  7.62it/s]Evaluation:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 56/57 [00:07<00:00,  7.61it/s]Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:07<00:00,  7.67it/s]
04/12/2021 07:10:18 - INFO - src.mtl_trainer -     eval_loss = 5.5155757017302935
04/12/2021 07:10:18 - INFO - src.mtl_trainer -     eval_matthews_corrcoef = 0.0
04/12/2021 07:10:18 - INFO - src.mtl_trainer -     eval_acc = 0.007170435741864313
04/12/2021 07:10:18 - INFO - src.mtl_trainer -     eval_f1_micro = 0.007170435741864313
04/12/2021 07:10:18 - INFO - src.mtl_trainer -     eval_f1_weighted = 0.00010209820881077336
04/12/2021 07:10:18 - INFO - src.mtl_trainer -     eval_acc_and_f1_weighted = 0.0036362669753375433
04/12/2021 07:10:18 - INFO - src.mtl_trainer -     epoch = 7.0
04/12/2021 07:10:18 - INFO - root -   *** Test ***
04/12/2021 07:10:18 - INFO - src.mtl_trainer -   D0
04/12/2021 07:10:18 - INFO - transformers.trainer -   ***** Running Prediction *****
04/12/2021 07:10:18 - INFO - transformers.trainer -     Num examples = 1836
04/12/2021 07:10:18 - INFO - transformers.trainer -     Batch size = 32
Prediction:   0%|                                        | 0/58 [00:00<?, ?it/s]Prediction:   2%|‚ñå                               | 1/58 [00:00<00:07,  7.66it/s]Prediction:   3%|‚ñà                               | 2/58 [00:00<00:07,  7.64it/s]Prediction:   5%|‚ñà‚ñã                              | 3/58 [00:00<00:07,  7.63it/s]Prediction:   7%|‚ñà‚ñà‚ñè                             | 4/58 [00:00<00:07,  7.64it/s]Prediction:   9%|‚ñà‚ñà‚ñä                             | 5/58 [00:00<00:06,  7.64it/s]Prediction:  10%|‚ñà‚ñà‚ñà‚ñé                            | 6/58 [00:00<00:06,  7.62it/s]Prediction:  12%|‚ñà‚ñà‚ñà‚ñä                            | 7/58 [00:00<00:06,  7.62it/s]Prediction:  14%|‚ñà‚ñà‚ñà‚ñà‚ñç                           | 8/58 [00:01<00:06,  7.66it/s]Prediction:  16%|‚ñà‚ñà‚ñà‚ñà‚ñâ                           | 9/58 [00:01<00:06,  7.66it/s]Prediction:  17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 10/58 [00:01<00:06,  7.64it/s]Prediction:  19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 11/58 [00:01<00:06,  7.63it/s]Prediction:  21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                        | 12/58 [00:01<00:06,  7.63it/s]Prediction:  22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                        | 13/58 [00:01<00:05,  7.63it/s]Prediction:  24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                       | 14/58 [00:01<00:05,  7.65it/s]Prediction:  26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 15/58 [00:01<00:05,  7.65it/s]Prediction:  28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 16/58 [00:02<00:05,  7.63it/s]Prediction:  29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      | 17/58 [00:02<00:05,  7.62it/s]Prediction:  31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 18/58 [00:02<00:05,  7.62it/s]Prediction:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 19/58 [00:02<00:05,  7.62it/s]Prediction:  34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 20/58 [00:02<00:04,  7.60it/s]Prediction:  36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 21/58 [00:02<00:04,  7.61it/s]Prediction:  38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 22/58 [00:02<00:04,  7.64it/s]Prediction:  40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                  | 23/58 [00:03<00:04,  7.64it/s]Prediction:  41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                  | 24/58 [00:03<00:04,  7.61it/s]Prediction:  43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                 | 25/58 [00:03<00:04,  7.62it/s]Prediction:  45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                 | 26/58 [00:03<00:04,  7.65it/s]Prediction:  47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 27/58 [00:03<00:04,  7.65it/s]Prediction:  48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 28/58 [00:03<00:03,  7.65it/s]Prediction:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 29/58 [00:03<00:03,  7.64it/s]Prediction:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 30/58 [00:03<00:03,  7.65it/s]Prediction:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 31/58 [00:04<00:03,  7.65it/s]Prediction:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 32/58 [00:04<00:03,  7.66it/s]Prediction:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 33/58 [00:04<00:03,  7.65it/s]Prediction:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 34/58 [00:04<00:03,  7.62it/s]Prediction:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 35/58 [00:04<00:03,  7.63it/s]Prediction:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 36/58 [00:04<00:02,  7.65it/s]Prediction:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 37/58 [00:04<00:02,  7.64it/s]Prediction:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 38/58 [00:04<00:02,  7.61it/s]Prediction:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 39/58 [00:05<00:02,  7.62it/s]Prediction:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 40/58 [00:05<00:02,  7.65it/s]Prediction:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 41/58 [00:05<00:02,  7.64it/s]Prediction:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 42/58 [00:05<00:02,  7.62it/s]Prediction:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 43/58 [00:05<00:01,  7.62it/s]Prediction:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 44/58 [00:05<00:01,  7.67it/s]Prediction:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 45/58 [00:05<00:01,  7.65it/s]Prediction:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 46/58 [00:06<00:01,  7.63it/s]Prediction:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 47/58 [00:06<00:01,  7.62it/s]Prediction:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 48/58 [00:06<00:01,  7.63it/s]Prediction:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 49/58 [00:06<00:01,  7.63it/s]Prediction:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 50/58 [00:06<00:01,  7.62it/s]Prediction:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 51/58 [00:06<00:00,  7.62it/s]Prediction:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 52/58 [00:06<00:00,  7.60it/s]Prediction:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 53/58 [00:06<00:00,  7.60it/s]Prediction:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 54/58 [00:07<00:00,  7.60it/s]Prediction:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 55/58 [00:07<00:00,  7.61it/s]Prediction:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 56/58 [00:07<00:00,  7.62it/s]Prediction:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 57/58 [00:07<00:00,  7.62it/s]Prediction: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 58/58 [00:07<00:00,  7.71it/s]
04/12/2021 07:10:25 - INFO - src.mtl_trainer -   ***** Test results D0 *****
04/12/2021 07:10:25 - INFO - src.mtl_trainer -   D1
04/12/2021 07:10:25 - INFO - transformers.trainer -   ***** Running Prediction *****
04/12/2021 07:10:25 - INFO - transformers.trainer -     Num examples = 1872
04/12/2021 07:10:25 - INFO - transformers.trainer -     Batch size = 32
Prediction:   0%|                                        | 0/59 [00:00<?, ?it/s]Prediction:   2%|‚ñå                               | 1/59 [00:00<00:07,  7.74it/s]Prediction:   3%|‚ñà                               | 2/59 [00:00<00:07,  7.71it/s]Prediction:   5%|‚ñà‚ñã                              | 3/59 [00:00<00:07,  7.67it/s]Prediction:   7%|‚ñà‚ñà‚ñè                             | 4/59 [00:00<00:07,  7.65it/s]Prediction:   8%|‚ñà‚ñà‚ñã                             | 5/59 [00:00<00:07,  7.62it/s]Prediction:  10%|‚ñà‚ñà‚ñà‚ñé                            | 6/59 [00:00<00:06,  7.62it/s]Prediction:  12%|‚ñà‚ñà‚ñà‚ñä                            | 7/59 [00:00<00:06,  7.63it/s]Prediction:  14%|‚ñà‚ñà‚ñà‚ñà‚ñé                           | 8/59 [00:01<00:06,  7.63it/s]Prediction:  15%|‚ñà‚ñà‚ñà‚ñà‚ñâ                           | 9/59 [00:01<00:06,  7.65it/s]Prediction:  17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 10/59 [00:01<00:06,  7.64it/s]Prediction:  19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                         | 11/59 [00:01<00:06,  7.66it/s]Prediction:  20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 12/59 [00:01<00:06,  7.64it/s]Prediction:  22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 13/59 [00:01<00:06,  7.62it/s]Prediction:  24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 14/59 [00:01<00:05,  7.64it/s]Prediction:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                       | 15/59 [00:01<00:05,  7.64it/s]Prediction:  27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 16/59 [00:02<00:05,  7.64it/s]Prediction:  29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 17/59 [00:02<00:05,  7.64it/s]Prediction:  31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 18/59 [00:02<00:05,  7.64it/s]Prediction:  32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 19/59 [00:02<00:05,  7.63it/s]Prediction:  34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 20/59 [00:02<00:05,  7.62it/s]Prediction:  36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 21/59 [00:02<00:04,  7.62it/s]Prediction:  37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 22/59 [00:02<00:04,  7.63it/s]Prediction:  39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 23/59 [00:03<00:04,  7.64it/s]Prediction:  41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 24/59 [00:03<00:04,  7.63it/s]Prediction:  42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 | 25/59 [00:03<00:04,  7.63it/s]Prediction:  44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 26/59 [00:03<00:04,  7.62it/s]Prediction:  46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                | 27/59 [00:03<00:04,  7.67it/s]Prediction:  47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 28/59 [00:03<00:04,  7.66it/s]Prediction:  49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 29/59 [00:03<00:03,  7.67it/s]Prediction:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 30/59 [00:03<00:03,  7.66it/s]Prediction:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 31/59 [00:04<00:03,  7.65it/s]Prediction:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 32/59 [00:04<00:03,  7.64it/s]Prediction:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 33/59 [00:04<00:03,  7.63it/s]Prediction:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 34/59 [00:04<00:03,  7.61it/s]Prediction:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 35/59 [00:04<00:03,  7.62it/s]Prediction:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 36/59 [00:04<00:03,  7.62it/s]Prediction:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 37/59 [00:04<00:02,  7.60it/s]Prediction:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 38/59 [00:04<00:02,  7.60it/s]Prediction:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç          | 39/59 [00:05<00:02,  7.63it/s]Prediction:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 40/59 [00:05<00:02,  7.63it/s]Prediction:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 41/59 [00:05<00:02,  7.61it/s]Prediction:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 42/59 [00:05<00:02,  7.62it/s]Prediction:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 43/59 [00:05<00:02,  7.64it/s]Prediction:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 44/59 [00:05<00:01,  7.64it/s]Prediction:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 45/59 [00:05<00:01,  7.65it/s]Prediction:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 46/59 [00:06<00:01,  7.65it/s]Prediction:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã      | 47/59 [00:06<00:01,  7.65it/s]Prediction:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 48/59 [00:06<00:01,  7.64it/s]Prediction:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 49/59 [00:06<00:01,  7.63it/s]Prediction:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 50/59 [00:06<00:01,  7.63it/s]Prediction:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 51/59 [00:06<00:01,  7.62it/s]Prediction:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 52/59 [00:06<00:00,  7.62it/s]Prediction:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 53/59 [00:06<00:00,  7.62it/s]Prediction:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 54/59 [00:07<00:00,  7.62it/s]Prediction:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 55/59 [00:07<00:00,  7.59it/s]Prediction:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 56/59 [00:07<00:00,  7.60it/s]Prediction:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 57/59 [00:07<00:00,  7.63it/s]Prediction:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 58/59 [00:07<00:00,  7.63it/s]Prediction: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 59/59 [00:07<00:00,  7.69it/s]
04/12/2021 07:10:33 - INFO - src.mtl_trainer -   ***** Test results D1 *****

{"eval_loss": 3.667119571140834, "eval_matthews_corrcoef": 0.0, "eval_acc": 0.1266891891891892, "eval_f1_micro": 0.1266891891891892, "eval_f1_weighted": 0.02849082215648932, "eval_acc_and_f1_weighted": 0.07759000567283926, "epoch": 7.0, "step": 28}
{"eval_loss": 5.5155757017302935, "eval_matthews_corrcoef": 0.0, "eval_acc": 0.007170435741864313, "eval_f1_micro": 0.007170435741864313, "eval_f1_weighted": 0.00010209820881077336, "eval_acc_and_f1_weighted": 0.0036362669753375433, "epoch": 7.0, "step": 28}
wandb: Waiting for W&B process to finish, PID 2474
wandb: Program ended successfully.
wandb: - 0.02MB of 0.02MB uploaded (0.00MB deduped)wandb: \ 0.02MB of 0.02MB uploaded (0.00MB deduped)wandb: | 0.02MB of 0.02MB uploaded (0.00MB deduped)wandb: / 0.02MB of 0.02MB uploaded (0.00MB deduped)wandb: - 0.02MB of 0.02MB uploaded (0.00MB deduped)wandb: \ 0.02MB of 0.02MB uploaded (0.00MB deduped)wandb: | 0.02MB of 0.02MB uploaded (0.00MB deduped)wandb: / 0.02MB of 0.02MB uploaded (0.00MB deduped)wandb: - 0.02MB of 0.02MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/datasci/CA-MTL/wandb/run-20210412_070953-2nzy21qj/logs/debug.log
wandb: Find internal logs for this run at: /home/datasci/CA-MTL/wandb/run-20210412_070953-2nzy21qj/logs/debug-internal.log
wandb: Run summary:
wandb:                  eval_loss 5.51558
wandb:     eval_matthews_corrcoef 0.0
wandb:                   eval_acc 0.00717
wandb:              eval_f1_micro 0.00717
wandb:           eval_f1_weighted 0.0001
wandb:   eval_acc_and_f1_weighted 0.00364
wandb:                      epoch 7.0
wandb:                   _runtime 40
wandb:                 _timestamp 1618236633
wandb:                      _step 28
wandb: Run history:
wandb:                  eval_loss ‚ñÅ
wandb:     eval_matthews_corrcoef ‚ñÅ
wandb:                   eval_acc ‚ñÅ
wandb:              eval_f1_micro ‚ñÅ
wandb:           eval_f1_weighted ‚ñÅ
wandb:   eval_acc_and_f1_weighted ‚ñÅ
wandb:                      epoch ‚ñÅ
wandb:                   _runtime ‚ñÅ
wandb:                 _timestamp ‚ñÅ
wandb:                      _step ‚ñÅ
wandb: 
wandb: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced solar-night-10: https://wandb.ai/heatxg/huggingface/runs/2nzy21qj

