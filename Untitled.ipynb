{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/huggingface/transformers/blob/v2.11.0/src/transformers/training_args.py  \n",
    "https://github.com/huggingface/transformers/blob/v2.11.0/src/transformers/trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing around with GLUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import glue_tasks_num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_tasks_num_labels['sst-2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(\"/hub/CA-MTL/glue_data/data/rte/test.jsonl\") as f:\n",
    "    for i in range(10):\n",
    "        print(next(f.iter()))\n",
    "#         print line['doi'] # or whatever else you'd like to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What should the input mimic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BertTokenizer\n",
    ")\n",
    "from transformers.data.processors import InputExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "text_a = ['this is one sentence', 'this is another', 'there are flowers in her hair', \n",
    "         'hair is weird', 'i am glad that it is spring time', 'things are really uncertain']\n",
    "label = [1, 2, 1, 1, 2, 2] \n",
    "for guid in range(len(label)):\n",
    "    examples.append(InputExample(guid=guid, text_a=text_a[guid], text_b=None, label=label[guid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "batch_encoding = tokenizer.batch_encode_plus(\n",
    "        [(example.text_a, example.text_b) for example in examples],\n",
    "        max_length=256,\n",
    "        pad_to_max_length=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " for i in range(len(examples)):\n",
    "        inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n",
    "        print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('.')\n",
    "from src.data.task_data_processors import task_processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'D1'\n",
    "date_tag = \"2021_04_08\"\n",
    "data_dir= f\"/hub/CA-MTL/data/{task}/{date_tag}\"\n",
    "set_type = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = task_processors[task]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = processor.get_test_examples(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.get_labels(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(processor.get_labels(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    set_seed,\n",
    "#     AutoTokenizer,\n",
    "#     AutoConfig,\n",
    "    EvalPrediction,\n",
    "    BertConfig, \n",
    "    BertTokenizer,\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "from src.model.ca_mtl import CaMtl, CaMtlArguments\n",
    "from src.utils.misc import MultiTaskDataArguments, Split\n",
    "from src.mtl_trainer import MultiTaskTrainer, MultiTaskTrainingArguments\n",
    "from src.data.mtl_dataset import MultiTaskDataset\n",
    "from src.data.task_dataset import TaskDataset\n",
    "\n",
    "# data_args = MultiTaskDataArguments(\n",
    "#     data_dir='/hub/CA-MTL/data', tasks=['D0', 'D1'], \n",
    "#     task_data_folders=['D0/2021_04_08', 'D1/2021_04_08'])\n",
    "data_args = MultiTaskDataArguments(\n",
    "    data_dir='/hub/CA-MTL/data', tasks=['D0', 'D1', 'LOC'], \n",
    "    task_data_folders=['D0/2021_04_08', 'D1/2021_04_08', 'LOC/2021_04_08'])\n",
    "model_args = CaMtlArguments(model_name_or_path='CA-MTL-tiny')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_dataset = MultiTaskDataset(data_args, tokenizer, limit_length=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, task in enumerate(data_args.tasks):\n",
    "    print(task)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(train_dataset):\n",
    "    if i == 10:\n",
    "        print(batch)\n",
    "    elif i == 3308:\n",
    "        print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How should I think about the DataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler, Sampler, SequentialSampler\n",
    "from transformers import (\n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    EvalPrediction, \n",
    "    DataCollator,\n",
    "    DefaultDataCollator,\n",
    ")\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "local_rank = -1\n",
    "train_batch_size = 32\n",
    "\n",
    "train_sampler = (\n",
    "    RandomSampler(train_dataset)\n",
    "    if local_rank == -1\n",
    "    else DistributedSampler(train_dataset)\n",
    ")\n",
    "data_collator = DefaultDataCollator()\n",
    "data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=train_batch_size,\n",
    "    sampler=train_sampler,\n",
    "    collate_fn=data_collator.collate_batch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(data_loader):\n",
    "    if i == 1000:\n",
    "        print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "m = torch.nn.Softmax(dim=1)\n",
    "input = np.random.rand(2, 3)\n",
    "input_t = torch.Tensor(input)\n",
    "output = m(torch.Tensor(input)).numpy()\n",
    "output_t = m(input_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "task_embedding = torch.nn.Embedding(5, 768)\n",
    "task_ids = torch.Tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 1, 0, 0]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_embedding(task_ids).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't work just yet because of missing arguments in the mt uncertainty sampling... waiting on feedback from the authors to help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "--save_steps\n",
    "--save_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python run.py \\\n",
    "--model_name_or_path CA-MTL-base \\\n",
    "--data_dir /hub/CA-MTL/data \\\n",
    "--output_dir /hub/CA-MTL/models \\\n",
    "--tasks D0 D1 \\\n",
    "--task_data_folders D0/2021_04_08 D1/2021_04_08 \\\n",
    "--do_train \\\n",
    "--evaluate_during_training \\\n",
    "--per_device_train_batch_size 32 \\\n",
    "--per_device_eval_batch_size 32 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--adam_epsilon 1e-8 \\\n",
    "--num_train_epochs 7 \\\n",
    "--warmup_steps 0 \\\n",
    "--save_steps 8000 \\\n",
    "--seed 43 \\\n",
    "--use_mt_uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks good at first but when we go to run we run out of GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python run.py \\\n",
    "--model_name_or_path CA-MTL-base \\\n",
    "--data_dir /hub/CA-MTL/data \\\n",
    "--output_dir /hub/CA-MTL/models \\\n",
    "--tasks D0 D1 \\\n",
    "--task_data_folders D0/2021_04_08 D1/2021_04_08 \\\n",
    "--do_train \\\n",
    "--evaluate_during_training \\\n",
    "--per_device_train_batch_size 32 \\\n",
    "--per_device_eval_batch_size 32 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--adam_epsilon 1e-8 \\\n",
    "--num_train_epochs 7 \\\n",
    "--warmup_steps 0 \\\n",
    "--save_steps 8000 \\\n",
    "--seed 43 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TinyBERT Run without Uncertainty Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python run.py \\\n",
    "--model_name_or_path CA-MTL-tiny \\\n",
    "--data_dir /hub/CA-MTL/data \\\n",
    "--output_dir /hub/CA-MTL/models \\\n",
    "--tasks D0 D1 \\\n",
    "--task_data_folders D0/2021_04_08 D1/2021_04_08 \\\n",
    "--do_train \\\n",
    "--evaluate_during_training \\\n",
    "--per_device_train_batch_size 32 \\\n",
    "--per_device_eval_batch_size 32 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--adam_epsilon 1e-8 \\\n",
    "--num_train_epochs 7 \\\n",
    "--warmup_steps 0 \\\n",
    "--save_steps 8000 \\\n",
    "--seed 43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TinyBERT Run without Uncertainty Sampling with Evaluation and Prediction\n",
    "- prediction works but I don't want to look at the test set... I need a way to look at the evaluation set in detail... evaluation data does not write out\n",
    "- this was an easy fix... I just replaced the \"dev\" set with the \"train_dev\" set and then used the \"dev\" set as the \"test\" set in the code... should have thought of this immediately... the \"test\" set can be evaluated easily in the future by switching this up by for now this allows us to get metrics on both... the \"test\" (i.e. : \"dev\" for us) set metrics do not log to wandb yet so I need to get that set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- batch sizes: 8, 16, 32, 64, 128\n",
    "- learning rates: 3e-4, 1e-4, 5e-5, 3e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python run.py \\\n",
    "--model_name_or_path CA-MTL-tiny \\\n",
    "--data_dir /hub/CA-MTL/data \\\n",
    "--output_dir /hub/CA-MTL/models \\\n",
    "--tasks D0 D1 \\\n",
    "--task_data_folders D0/2021_04_08 D1/2021_04_08 \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_predict \\\n",
    "--evaluate_during_training \\\n",
    "--per_device_train_batch_size 32 \\\n",
    "--per_device_eval_batch_size 32 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--adam_epsilon 1e-8 \\\n",
    "--num_train_epochs 7 \\\n",
    "--warmup_steps 0 \\\n",
    "--save_steps 8000 \\\n",
    "--seed 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python run.py \\\n",
    "--model_name_or_path CA-MTL-tiny \\\n",
    "--data_dir /hub/CA-MTL/data \\\n",
    "--output_dir /hub/CA-MTL/models \\\n",
    "--tasks D0 D1 \\\n",
    "--task_data_folders D0/2021_04_08 D1/2021_04_08 \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_predict \\\n",
    "--evaluate_during_training \\\n",
    "--per_device_train_batch_size 32 \\\n",
    "--per_device_eval_batch_size 32 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--adam_epsilon 1e-8 \\\n",
    "--num_train_epochs 14 \\\n",
    "--warmup_steps 0 \\\n",
    "--save_steps 8000 \\\n",
    "--seed 1025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: If you run with a limit length, your .lock files remain and the data won't be regenerated... you have to delete the .lock files and cached* directories first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: prediction files get overwritten as well... naming has to be controlled somewhere so I will need to figure out where that is and create a way to control it... they also always say \"_test_\" which I do not want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python run.py \\\n",
    "--model_name_or_path CA-MTL-base \\\n",
    "--data_dir /hub/CA-MTL/data \\\n",
    "--output_dir /hub/CA-MTL/models \\\n",
    "--tasks D0 D1 \\\n",
    "--task_data_folders D0/2021_04_08 D1/2021_04_08 \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_predict \\\n",
    "--evaluate_during_training \\\n",
    "--per_device_train_batch_size 32 \\\n",
    "--per_device_eval_batch_size 32 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--adam_epsilon 1e-8 \\\n",
    "--num_train_epochs 7 \\\n",
    "--warmup_steps 0 \\\n",
    "--save_steps 8000 \\\n",
    "--seed 43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run without CUDA to look for BERT model load issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python run.py \\\n",
    "--model_name_or_path CA-MTL-base \\\n",
    "--data_dir /hub/CA-MTL/data \\\n",
    "--output_dir /hub/CA-MTL/models \\\n",
    "--tasks D0 D1 \\\n",
    "--max_seq_length 128 \\\n",
    "--task_data_folders D0/2021_04_08 D1/2021_04_08 \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_predict \\\n",
    "--evaluate_during_training \\\n",
    "--per_device_train_batch_size 32 \\\n",
    "--per_device_eval_batch_size 32 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--adam_epsilon 1e-8 \\\n",
    "--num_train_epochs 7 \\\n",
    "--warmup_steps 0 \\\n",
    "--save_steps 8000 \\\n",
    "--seed 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python run.py \\\n",
    "--model_name_or_path CA-MTL-base \\\n",
    "--data_dir /hub/CA-MTL/data \\\n",
    "--output_dir /hub/CA-MTL/models \\\n",
    "--tasks D1 \\\n",
    "--overwrite_cache \\\n",
    "--max_seq_length 128 \\\n",
    "--task_data_folders D1/2021_04_08 \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_predict \\\n",
    "--evaluate_during_training \\\n",
    "--per_device_train_batch_size 32 \\\n",
    "--per_device_eval_batch_size 32 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--adam_epsilon 1e-8 \\\n",
    "--num_train_epochs 7 \\\n",
    "--warmup_steps 0 \\\n",
    "--save_steps 8000 \\\n",
    "--seed 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python run.py \\\n",
    "--model_name_or_path CA-MTL-base-uncased \\\n",
    "--data_dir /hub/CA-MTL/data \\\n",
    "--output_dir /hub/CA-MTL/models \\\n",
    "--tasks D0 D1 \\\n",
    "--max_seq_length 128 \\\n",
    "--task_data_folders D0/2021_04_08 D1/2021_04_08 \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_predict \\\n",
    "--evaluate_during_training \\\n",
    "--per_device_train_batch_size 32 \\\n",
    "--per_device_eval_batch_size 32 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--adam_epsilon 1e-8 \\\n",
    "--num_train_epochs 7 \\\n",
    "--warmup_steps 0 \\\n",
    "--save_steps 8000 \\\n",
    "--seed 43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retesting uncertainty sampling after author update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python run.py \\\n",
    "--model_name_or_path CA-MTL-tiny \\\n",
    "--data_dir /hub/CA-MTL/data \\\n",
    "--output_dir /hub/CA-MTL/models \\\n",
    "--tasks D0 D1 \\\n",
    "--task_data_folders D0/2021_04_08 D1/2021_04_08 \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_predict \\\n",
    "--evaluate_during_training \\\n",
    "--per_device_train_batch_size 32 \\\n",
    "--per_device_eval_batch_size 32 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--adam_epsilon 1e-8 \\\n",
    "--num_train_epochs 7 \\\n",
    "--warmup_steps 0 \\\n",
    "--save_steps 8000 \\\n",
    "--seed 43 \\\n",
    "--use_mt_uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Runs\n",
    "- HAVE TO OVERWRITE THE CACHE ANY TIME YOU MAKE A CHANGE TO THE TASKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python run.py \\\n",
    "--model_name_or_path CA-MTL-tiny \\\n",
    "--data_dir /hub/CA-MTL/data \\\n",
    "--output_dir /hub/CA-MTL/models \\\n",
    "--tasks D0 \\\n",
    "--overwrite_cache \\\n",
    "--task_data_folders D0/2021_04_08 \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_predict \\\n",
    "--evaluate_during_training \\\n",
    "--per_device_train_batch_size 32 \\\n",
    "--per_device_eval_batch_size 32 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--adam_epsilon 1e-8 \\\n",
    "--num_train_epochs 7 \\\n",
    "--warmup_steps 0 \\\n",
    "--save_steps 8000 \\\n",
    "--seed 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python run.py \\\n",
    "--model_name_or_path CA-MTL-tiny \\\n",
    "--data_dir /hub/CA-MTL/data \\\n",
    "--output_dir /hub/CA-MTL/models \\\n",
    "--tasks D1 \\\n",
    "--overwrite_cache \\\n",
    "--task_data_folders D1/2021_04_08 \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_predict \\\n",
    "--evaluate_during_training \\\n",
    "--per_device_train_batch_size 32 \\\n",
    "--per_device_eval_batch_size 32 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--adam_epsilon 1e-8 \\\n",
    "--num_train_epochs 7 \\\n",
    "--warmup_steps 0 \\\n",
    "--save_steps 8000 \\\n",
    "--seed 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python run.py --model_name_or_path CA-MTL-base-uncased --data_dir /hub/CA-MTL/data --output_dir /hub/CA-MTL/models --tasks D1 --overwrite_cache --max_seq_length 128 --task_data_folders D1/2021_04_08 --do_train --do_eval --do_predict --evaluate_during_training --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --learning_rate 5e-5 --adam_epsilon 1e-8 --num_train_epochs 7 --warmup_steps 100 --save_steps 8000 --seed 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python run.py --model_name_or_path bert-base-uncased --data_dir /hub/CA-MTL/data --output_dir /hub/CA-MTL/models --tasks D0 D1 --overwrite_cache --max_seq_length 128 --task_data_folders D0/2021_04_08 D1/2021_04_08 --do_train --do_eval --do_predict --evaluate_during_training --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --learning_rate 5e-5 --adam_epsilon 1e-8 --num_train_epochs 7 --warmup_steps 0 --save_steps 8000 --seed 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python run.py --model_name_or_path bert-base-uncased --data_dir /hub/CA-MTL/data --output_dir /hub/CA-MTL/models --tasks D1 --overwrite_cache --max_seq_length 128 --task_data_folders D1/2021_04_08 --do_train --do_eval --do_predict --evaluate_during_training --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --learning_rate 5e-5 --adam_epsilon 1e-8 --num_train_epochs 7 --warmup_steps 0 --save_steps 8000 --seed 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python run.py --model_name_or_path bert-base-cased --data_dir /hub/CA-MTL/data --output_dir /hub/CA-MTL/models --tasks D0 D1 --overwrite_cache --max_seq_length 128 --task_data_folders D0/2021_04_08 D1/2021_04_08 --do_train --do_eval --do_predict --evaluate_during_training --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --learning_rate 5e-5 --adam_epsilon 1e-8 --num_train_epochs 7 --warmup_steps 0 --save_steps 8000 --seed 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python run_stl.py --model_name_or_path distilbert-base-uncased --data_dir /hub/CA-MTL/data --output_dir /hub/CA-MTL/models --tasks D0 --overwrite_cache --max_seq_length 128 --task_data_folders D0/2021_04_08 --do_train --do_eval --do_predict --evaluate_during_training --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --learning_rate 5e-5 --adam_epsilon 1e-8 --num_train_epochs 7 --warmup_steps 0 --save_steps 8000 --seed 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.py --model_name_or_path CA-MTL-tiny --encoder_type CA-MTL-tiny --data_dir /hub/CA-MTL/data --output_dir /hub/CA-MTL/mock_models --tasks D0 D1 MANC LOC SIGNT --overwrite_cache --task_data_folders D0/2021_04_08 D1/2021_04_08 MANC/2021_04_08 LOC/2021_04_08 SIGNT/2021_04_08 --do_train --do_eval --do_predict --evaluate_during_training --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --learning_rate 5e-5 --adam_epsilon 1e-8 --num_train_epochs 7 --warmup_steps 0 --save_steps 1500 --save_total_limit 1 --seed 43\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ca-mtl-env",
   "language": "python",
   "name": "ca-mtl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
