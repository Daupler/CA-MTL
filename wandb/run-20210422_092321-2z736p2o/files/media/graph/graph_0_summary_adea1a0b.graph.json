{"format": "torch", "nodes": [{"name": "bert", "id": 140091275685648, "class_name": "CaMtlBaseEncoder(\n  (task_type_embeddings): Embedding(5, 768)\n  (conditional_alignment): FiLM(\n    (gb_weights): Linear(in_features=768, out_features=1536, bias=True)\n  )\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (token_type_embeddings): Embedding(2, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): MyBertEncoder9(\n    (task_transformation): Linear(in_features=768, out_features=768, bias=True)\n    (layer): ModuleList(\n      (0): BertLayer9(\n        (attention): MyBertAttention9(\n          (self): MyBertSelfAttention9(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (cond_block_diag_attn): CBDA(\n              (gb_weights): Linear(in_features=768, out_features=172, bias=True)\n            )\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): BertLayer9(\n        (attention): MyBertAttention9(\n          (self): MyBertSelfAttention9(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (cond_block_diag_attn): CBDA(\n              (gb_weights): Linear(in_features=768, out_features=172, bias=True)\n            )\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): BertLayer9(\n        (attention): MyBertAttention9(\n          (self): MyBertSelfAttention9(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (cond_block_diag_attn): CBDA(\n              (gb_weights): Linear(in_features=768, out_features=172, bias=True)\n            )\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): MyBertLayer9(\n        (attention): MyBertAttention9(\n          (self): MyBertSelfAttention9(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (cond_block_diag_attn): CBDA(\n              (gb_weights): Linear(in_features=768, out_features=172, bias=True)\n            )\n          )\n          (output): MyBertSelfOutput9(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): ConditionalLayerNorm(\n              (768,), 768, eps=1e-12\n              (ln_weight_modulation): FiLM(\n                (gb_weights): Linear(in_features=768, out_features=1536, bias=True)\n              )\n            )\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): MyBertOutput9(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): ConditionalLayerNorm(\n            (768,), 768, eps=1e-12\n            (ln_weight_modulation): FiLM(\n              (gb_weights): Linear(in_features=768, out_features=1536, bias=True)\n            )\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): MyBertLayer9(\n        (attention): MyBertAttention9(\n          (self): MyBertSelfAttention9(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (cond_block_diag_attn): CBDA(\n              (gb_weights): Linear(in_features=768, out_features=172, bias=True)\n            )\n          )\n          (output): MyBertSelfOutput9(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): ConditionalLayerNorm(\n              (768,), 768, eps=1e-12\n              (ln_weight_modulation): FiLM(\n                (gb_weights): Linear(in_features=768, out_features=1536, bias=True)\n              )\n            )\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): MyBertOutput9(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): ConditionalLayerNorm(\n            (768,), 768, eps=1e-12\n            (ln_weight_modulation): FiLM(\n              (gb_weights): Linear(in_features=768, out_features=1536, bias=True)\n            )\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): MyBertAdapterLayer9(\n        (new_attention): MyBertAttention9(\n          (self): MyBertSelfAttention9(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (cond_block_diag_attn): CBDA(\n              (gb_weights): Linear(in_features=768, out_features=172, bias=True)\n            )\n          )\n          (output): MyBertSelfOutput9(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): ConditionalLayerNorm(\n              (768,), 768, eps=1e-12\n              (ln_weight_modulation): FiLM(\n                (gb_weights): Linear(in_features=768, out_features=1536, bias=True)\n              )\n            )\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (new_intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (new_output): MyBertOutput9(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): ConditionalLayerNorm(\n            (768,), 768, eps=1e-12\n            (ln_weight_modulation): FiLM(\n              (gb_weights): Linear(in_features=768, out_features=1536, bias=True)\n            )\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (adapter): BertAdapter9(\n          (bottleneck): ConditionalBottleNeck(\n            (emb_transf): Linear(in_features=768, out_features=768, bias=True)\n            (hidden_modulation): FiLM(\n              (gb_weights): Linear(in_features=768, out_features=1536, bias=True)\n            )\n            (down_proj_layer): Linear(in_features=768, out_features=256, bias=True)\n            (up_proj_layer): Linear(in_features=256, out_features=768, bias=True)\n          )\n          (condlayernorm): ConditionalLayerNorm(\n            (768,), 768, eps=1e-12\n            (ln_weight_modulation): FiLM(\n              (gb_weights): Linear(in_features=768, out_features=1536, bias=True)\n            )\n          )\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)", "parameters": [["task_type_embeddings.weight", [5, 768]], ["conditional_alignment.gb_weights.weight", [1536, 768]], ["conditional_alignment.gb_weights.bias", [1536]], ["embeddings.word_embeddings.weight", [30522, 768]], ["embeddings.position_embeddings.weight", [512, 768]], ["embeddings.token_type_embeddings.weight", [2, 768]], ["embeddings.LayerNorm.weight", [768]], ["embeddings.LayerNorm.bias", [768]], ["encoder.task_transformation.weight", [768, 768]], ["encoder.task_transformation.bias", [768]], ["encoder.layer.0.attention.self.random_weight_matrix", [256, 86]], ["encoder.layer.0.attention.self.query.weight", [768, 768]], ["encoder.layer.0.attention.self.query.bias", [768]], ["encoder.layer.0.attention.self.key.weight", [768, 768]], ["encoder.layer.0.attention.self.key.bias", [768]], ["encoder.layer.0.attention.self.value.weight", [768, 768]], ["encoder.layer.0.attention.self.value.bias", [768]], ["encoder.layer.0.attention.self.cond_block_diag_attn.gb_weights.weight", [172, 768]], ["encoder.layer.0.attention.self.cond_block_diag_attn.gb_weights.bias", [172]], ["encoder.layer.0.attention.output.dense.weight", [768, 768]], ["encoder.layer.0.attention.output.dense.bias", [768]], ["encoder.layer.0.attention.output.LayerNorm.weight", [768]], ["encoder.layer.0.attention.output.LayerNorm.bias", [768]], ["encoder.layer.0.intermediate.dense.weight", [3072, 768]], ["encoder.layer.0.intermediate.dense.bias", [3072]], ["encoder.layer.0.output.dense.weight", [768, 3072]], ["encoder.layer.0.output.dense.bias", [768]], ["encoder.layer.0.output.LayerNorm.weight", [768]], ["encoder.layer.0.output.LayerNorm.bias", [768]], ["encoder.layer.1.attention.self.random_weight_matrix", [256, 86]], ["encoder.layer.1.attention.self.query.weight", [768, 768]], ["encoder.layer.1.attention.self.query.bias", [768]], ["encoder.layer.1.attention.self.key.weight", [768, 768]], ["encoder.layer.1.attention.self.key.bias", [768]], ["encoder.layer.1.attention.self.value.weight", [768, 768]], ["encoder.layer.1.attention.self.value.bias", [768]], ["encoder.layer.1.attention.self.cond_block_diag_attn.gb_weights.weight", [172, 768]], ["encoder.layer.1.attention.self.cond_block_diag_attn.gb_weights.bias", [172]], ["encoder.layer.1.attention.output.dense.weight", [768, 768]], ["encoder.layer.1.attention.output.dense.bias", [768]], ["encoder.layer.1.attention.output.LayerNorm.weight", [768]], ["encoder.layer.1.attention.output.LayerNorm.bias", [768]], ["encoder.layer.1.intermediate.dense.weight", [3072, 768]], ["encoder.layer.1.intermediate.dense.bias", [3072]], ["encoder.layer.1.output.dense.weight", [768, 3072]], ["encoder.layer.1.output.dense.bias", [768]], ["encoder.layer.1.output.LayerNorm.weight", [768]], ["encoder.layer.1.output.LayerNorm.bias", [768]], ["encoder.layer.2.attention.self.random_weight_matrix", [256, 86]], ["encoder.layer.2.attention.self.query.weight", [768, 768]], ["encoder.layer.2.attention.self.query.bias", [768]], ["encoder.layer.2.attention.self.key.weight", [768, 768]], ["encoder.layer.2.attention.self.key.bias", [768]], ["encoder.layer.2.attention.self.value.weight", [768, 768]], ["encoder.layer.2.attention.self.value.bias", [768]], ["encoder.layer.2.attention.self.cond_block_diag_attn.gb_weights.weight", [172, 768]], ["encoder.layer.2.attention.self.cond_block_diag_attn.gb_weights.bias", [172]], ["encoder.layer.2.attention.output.dense.weight", [768, 768]], ["encoder.layer.2.attention.output.dense.bias", [768]], ["encoder.layer.2.attention.output.LayerNorm.weight", [768]], ["encoder.layer.2.attention.output.LayerNorm.bias", [768]], ["encoder.layer.2.intermediate.dense.weight", [3072, 768]], ["encoder.layer.2.intermediate.dense.bias", [3072]], ["encoder.layer.2.output.dense.weight", [768, 3072]], ["encoder.layer.2.output.dense.bias", [768]], ["encoder.layer.2.output.LayerNorm.weight", [768]], ["encoder.layer.2.output.LayerNorm.bias", [768]], ["encoder.layer.3.attention.self.random_weight_matrix", [256, 86]], ["encoder.layer.3.attention.self.query.weight", [768, 768]], ["encoder.layer.3.attention.self.query.bias", [768]], ["encoder.layer.3.attention.self.key.weight", [768, 768]], ["encoder.layer.3.attention.self.key.bias", [768]], ["encoder.layer.3.attention.self.value.weight", [768, 768]], ["encoder.layer.3.attention.self.value.bias", [768]], ["encoder.layer.3.attention.self.cond_block_diag_attn.gb_weights.weight", [172, 768]], ["encoder.layer.3.attention.self.cond_block_diag_attn.gb_weights.bias", [172]], ["encoder.layer.3.attention.output.dense.weight", [768, 768]], ["encoder.layer.3.attention.output.dense.bias", [768]], ["encoder.layer.3.attention.output.LayerNorm.weight", [768]], ["encoder.layer.3.attention.output.LayerNorm.bias", [768]], ["encoder.layer.3.attention.output.LayerNorm.ln_weight_modulation.gb_weights.weight", [1536, 768]], ["encoder.layer.3.attention.output.LayerNorm.ln_weight_modulation.gb_weights.bias", [1536]], ["encoder.layer.3.intermediate.dense.weight", [3072, 768]], ["encoder.layer.3.intermediate.dense.bias", [3072]], ["encoder.layer.3.output.dense.weight", [768, 3072]], ["encoder.layer.3.output.dense.bias", [768]], ["encoder.layer.3.output.LayerNorm.weight", [768]], ["encoder.layer.3.output.LayerNorm.bias", [768]], ["encoder.layer.3.output.LayerNorm.ln_weight_modulation.gb_weights.weight", [1536, 768]], ["encoder.layer.3.output.LayerNorm.ln_weight_modulation.gb_weights.bias", [1536]], ["encoder.layer.4.attention.self.random_weight_matrix", [256, 86]], ["encoder.layer.4.attention.self.query.weight", [768, 768]], ["encoder.layer.4.attention.self.query.bias", [768]], ["encoder.layer.4.attention.self.key.weight", [768, 768]], ["encoder.layer.4.attention.self.key.bias", [768]], ["encoder.layer.4.attention.self.value.weight", [768, 768]], ["encoder.layer.4.attention.self.value.bias", [768]], ["encoder.layer.4.attention.self.cond_block_diag_attn.gb_weights.weight", [172, 768]], ["encoder.layer.4.attention.self.cond_block_diag_attn.gb_weights.bias", [172]], ["encoder.layer.4.attention.output.dense.weight", [768, 768]], ["encoder.layer.4.attention.output.dense.bias", [768]], ["encoder.layer.4.attention.output.LayerNorm.weight", [768]], ["encoder.layer.4.attention.output.LayerNorm.bias", [768]], ["encoder.layer.4.attention.output.LayerNorm.ln_weight_modulation.gb_weights.weight", [1536, 768]], ["encoder.layer.4.attention.output.LayerNorm.ln_weight_modulation.gb_weights.bias", [1536]], ["encoder.layer.4.intermediate.dense.weight", [3072, 768]], ["encoder.layer.4.intermediate.dense.bias", [3072]], ["encoder.layer.4.output.dense.weight", [768, 3072]], ["encoder.layer.4.output.dense.bias", [768]], ["encoder.layer.4.output.LayerNorm.weight", [768]], ["encoder.layer.4.output.LayerNorm.bias", [768]], ["encoder.layer.4.output.LayerNorm.ln_weight_modulation.gb_weights.weight", [1536, 768]], ["encoder.layer.4.output.LayerNorm.ln_weight_modulation.gb_weights.bias", [1536]], ["encoder.layer.5.new_attention.self.random_weight_matrix", [256, 86]], ["encoder.layer.5.new_attention.self.query.weight", [768, 768]], ["encoder.layer.5.new_attention.self.query.bias", [768]], ["encoder.layer.5.new_attention.self.key.weight", [768, 768]], ["encoder.layer.5.new_attention.self.key.bias", [768]], ["encoder.layer.5.new_attention.self.value.weight", [768, 768]], ["encoder.layer.5.new_attention.self.value.bias", [768]], ["encoder.layer.5.new_attention.self.cond_block_diag_attn.gb_weights.weight", [172, 768]], ["encoder.layer.5.new_attention.self.cond_block_diag_attn.gb_weights.bias", [172]], ["encoder.layer.5.new_attention.output.dense.weight", [768, 768]], ["encoder.layer.5.new_attention.output.dense.bias", [768]], ["encoder.layer.5.new_attention.output.LayerNorm.weight", [768]], ["encoder.layer.5.new_attention.output.LayerNorm.bias", [768]], ["encoder.layer.5.new_attention.output.LayerNorm.ln_weight_modulation.gb_weights.weight", [1536, 768]], ["encoder.layer.5.new_attention.output.LayerNorm.ln_weight_modulation.gb_weights.bias", [1536]], ["encoder.layer.5.new_intermediate.dense.weight", [3072, 768]], ["encoder.layer.5.new_intermediate.dense.bias", [3072]], ["encoder.layer.5.new_output.dense.weight", [768, 3072]], ["encoder.layer.5.new_output.dense.bias", [768]], ["encoder.layer.5.new_output.LayerNorm.weight", [768]], ["encoder.layer.5.new_output.LayerNorm.bias", [768]], ["encoder.layer.5.new_output.LayerNorm.ln_weight_modulation.gb_weights.weight", [1536, 768]], ["encoder.layer.5.new_output.LayerNorm.ln_weight_modulation.gb_weights.bias", [1536]], ["encoder.layer.5.adapter.bottleneck.emb_transf.weight", [768, 768]], ["encoder.layer.5.adapter.bottleneck.emb_transf.bias", [768]], ["encoder.layer.5.adapter.bottleneck.hidden_modulation.gb_weights.weight", [1536, 768]], ["encoder.layer.5.adapter.bottleneck.hidden_modulation.gb_weights.bias", [1536]], ["encoder.layer.5.adapter.bottleneck.down_proj_layer.weight", [256, 768]], ["encoder.layer.5.adapter.bottleneck.down_proj_layer.bias", [256]], ["encoder.layer.5.adapter.bottleneck.up_proj_layer.weight", [768, 256]], ["encoder.layer.5.adapter.bottleneck.up_proj_layer.bias", [768]], ["encoder.layer.5.adapter.condlayernorm.weight", [768]], ["encoder.layer.5.adapter.condlayernorm.bias", [768]], ["encoder.layer.5.adapter.condlayernorm.ln_weight_modulation.gb_weights.weight", [1536, 768]], ["encoder.layer.5.adapter.condlayernorm.ln_weight_modulation.gb_weights.bias", [1536]], ["pooler.dense.weight", [768, 768]], ["pooler.dense.bias", [768]]], "output_shape": [[32, 256, 768], [32, 768]], "num_parameters": [3840, 1179648, 1536, 23440896, 393216, 1536, 768, 768, 589824, 768, 22016, 589824, 768, 589824, 768, 589824, 768, 132096, 172, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 22016, 589824, 768, 589824, 768, 589824, 768, 132096, 172, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 22016, 589824, 768, 589824, 768, 589824, 768, 132096, 172, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 22016, 589824, 768, 589824, 768, 589824, 768, 132096, 172, 589824, 768, 768, 768, 1179648, 1536, 2359296, 3072, 2359296, 768, 768, 768, 1179648, 1536, 22016, 589824, 768, 589824, 768, 589824, 768, 132096, 172, 589824, 768, 768, 768, 1179648, 1536, 2359296, 3072, 2359296, 768, 768, 768, 1179648, 1536, 22016, 589824, 768, 589824, 768, 589824, 768, 132096, 172, 589824, 768, 768, 768, 1179648, 1536, 2359296, 3072, 2359296, 768, 768, 768, 1179648, 1536, 589824, 768, 1179648, 1536, 196608, 256, 196608, 768, 768, 768, 1179648, 1536, 589824, 768]}], "edges": []}