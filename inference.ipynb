{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    set_seed,\n",
    "    EvalPrediction,\n",
    "    BertConfig, \n",
    "    BertTokenizer\n",
    ")\n",
    "\n",
    "from src.model.ca_mtl import CaMtl, CaMtlArguments\n",
    "from src.utils.misc import MultiTaskDataArguments, Split\n",
    "from src.mtl_trainer import MultiTaskTrainer, MultiTaskTrainingArguments\n",
    "from src.data.mtl_dataset import MultiTaskDataset\n",
    "from src.data.task_dataset import TaskDataset\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    set_seed,\n",
    "    EvalPrediction,\n",
    "    BertConfig, \n",
    "    BertTokenizer\n",
    ")\n",
    "\n",
    "from src.model.ca_mtl import CaMtl, CaMtlArguments\n",
    "from src.utils.misc import MultiTaskDataArguments, Split\n",
    "from src.mtl_trainer import MultiTaskTrainer, MultiTaskTrainingArguments\n",
    "from src.data.mtl_dataset import MultiTaskDataset\n",
    "from src.data.task_dataset import TaskDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "python run.py \\\n",
    "--model_name_or_path CA-MTL-tiny \\\n",
    "--data_dir /hub/CA-MTL/data \\\n",
    "--output_dir /hub/CA-MTL/mock_models \\\n",
    "--tasks D0 D1 MANC LOC SIGNT \\\n",
    "--overwrite_cache \\\n",
    "--task_data_folders D0/2021_04_08 D1/2021_04_08 MANC/2021_04_08 LOC/2021_04_08 SIGNT/2021_04_08 \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_predict \\\n",
    "--evaluate_during_training \\\n",
    "--per_device_train_batch_size 32 \\\n",
    "--per_device_eval_batch_size 32 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--adam_epsilon 1e-8 \\\n",
    "--num_train_epochs 7 \\\n",
    "--warmup_steps 0 \\\n",
    "--save_steps 1500 \\\n",
    "--save_total_limit 1 \\\n",
    "--seed 43\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "python run.py \\\n",
    "--model_name_or_path CA-MTL-base \\\n",
    "--data_dir /hub/CA-MTL/data \\\n",
    "--output_dir /hub/CA-MTL/mock_models \\\n",
    "--tasks D0 D1 MANC LOC SIGNT \\\n",
    "--overwrite_cache \\\n",
    "--task_data_folders D0/2021_04_08 D1/2021_04_08 MANC/2021_04_08 LOC/2021_04_08 SIGNT/2021_04_08 \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_predict \\\n",
    "--evaluate_during_training \\\n",
    "--per_device_train_batch_size 16 \\\n",
    "--per_device_eval_batch_size 16 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--adam_epsilon 1e-8 \\\n",
    "--num_train_epochs 7 \\\n",
    "--warmup_steps 0 \\\n",
    "--save_steps 1500 \\\n",
    "--save_total_limit 1 \\\n",
    "--seed 43\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "python run_inference.py \\\n",
    "--model_name_or_path /hub/CA-MTL/mock_models/vital-smoke-40-9000 \\\n",
    "--data_dir /hub/CA-MTL/data \\\n",
    "--output_dir /hub/CA-MTL/data/SCORED \\\n",
    "--overwrite_cache \\\n",
    "--task_data_folders TOSCORE \\\n",
    "--do_predict\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "run.py --model_name_or_path CA-MTL-tiny --data_dir /hub/CA-MTL/data --output_dir /hub/CA-MTL/mock_models --tasks D0 D1 MANC LOC SIGNT --overwrite_cache --task_data_folders D0/2021_04_08 D1/2021_04_08 MANC/2021_04_08 LOC/2021_04_08 SIGNT/2021_04_08 --do_train --do_eval --do_predict --evaluate_during_training --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --learning_rate 5e-5 --adam_epsilon 1e-8 --num_train_epochs 7 --warmup_steps 0 --save_steps 10 --save_total_limit 1 --seed 43\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that predictions do not change when changing the model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/hub/CA-MTL/mock_models\"\n",
    "tasks = ['D0', 'D1', 'LOC', 'MANC', 'SIGNT']\n",
    "orig_model_run = \"leafy-sky-42\"\n",
    "new_model_run = \"zesty-glitter-82\"\n",
    "orig = {}\n",
    "new = {}\n",
    "for task in tasks:\n",
    "    orig[task] = pd.read_csv(f\"{model_dir}/{task}_test_iter_{orig_model_run}.tsv\", \n",
    "                             sep=\"\\t\")\n",
    "    new[task] = pd.read_csv(f\"{model_dir}/{task}_test_iter_{new_model_run}.tsv\",\n",
    "                            sep=\"\\t\")                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_task = \"SIGNT\"\n",
    "print((orig[check_task]['probability'] == new[check_task]['probability']).mean())\n",
    "display(orig[check_task].head(5))\n",
    "display(new[check_task].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run = \"leafy-sky-42\"\n",
    "model_args = CaMtlArguments(\n",
    "    model_name_or_path=f'/hub/CA-MTL/mock_models/{model_run}-9000', \n",
    "    encoder_type=\"CA-MTL-tiny\")\n",
    "\n",
    "data_args = MultiTaskDataArguments(\n",
    "    data_dir='/hub/CA-MTL/data',\n",
    "    overwrite_cache = True,\n",
    "    task_data_folders=['TOSCORE'])\n",
    "\n",
    "training_args = MultiTaskTrainingArguments(\n",
    "    output_dir='/hub/CA-MTL/data/SCORED', \n",
    "    overwrite_output_dir=False,\n",
    "    do_train=False, do_eval=False,\n",
    "    do_predict=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "def setup_logging(training_args):\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
    "    )\n",
    "    logger.warning(\n",
    "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "        training_args.local_rank,\n",
    "        training_args.device,\n",
    "        training_args.n_gpu,\n",
    "        bool(training_args.local_rank != -1),\n",
    "        training_args.fp16,\n",
    "    )\n",
    "def create_eval_datasets(mode, data_args, tokenizer, model_metadata):\n",
    "    eval_datasets = {}\n",
    "    for task_id, task_name in enumerate(model_metadata['tasks']):\n",
    "        eval_datasets[task_name] = TaskDataset(\n",
    "            task_name, task_id, data_args, tokenizer, mode=mode, \n",
    "            label_list=model_metadata['label_set'][task_name]\n",
    "        )\n",
    "    return eval_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "setup_logging(training_args)\n",
    "\n",
    "set_seed(training_args.seed)\n",
    "logger.info(training_args)\n",
    "\n",
    "model_metadata = json.load(open(f\"{model_args.model_name_or_path}/metadata.json\", 'r'))\n",
    "\n",
    "# update arguments with condition at model training\n",
    "data_args.max_seq_length = model_metadata['max_seq_length']\n",
    "num_tasks = len(model_metadata['label_set'])\n",
    "data_args.task_data_folders = data_args.task_data_folders*num_tasks\n",
    "data_args.tasks = model_metadata['tasks']\n",
    "model_args.encoder_type = model_metadata['model_name_or_path']\n",
    "\n",
    "config = BertConfig.from_pretrained(model_args.model_name_or_path)\n",
    "\n",
    "model = CaMtl.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    model_args,\n",
    "    data_args,\n",
    "    config=config)\n",
    "\n",
    "logger.info(model)\n",
    "\n",
    "# load the tokenizer that was used when the model was trained\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    CaMtl.get_base_model(model_metadata['model_name_or_path']),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s_model = torch.jit.script(CaMtl.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    model_args,\n",
    "    data_args,\n",
    "    config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load original test data\n",
    "# load original test data scores\n",
    "# load deployment test data sample\n",
    "# score test sample with model\n",
    "# score test sample with serialized model\n",
    "# compare all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def predict(\n",
    "#         self,\n",
    "#         eval_dataset: Optional[Dataset] = None,\n",
    "#         prediction_loss_only: Optional[bool] = None,\n",
    "#         scoring_model: Optional[str] = None\n",
    "#     ):\n",
    "#         logging.info(\"*** Test ***\")\n",
    "#         datasets = eval_dataset or self.test_datasets\n",
    "#         for task_name, test_dataset in datasets.items():\n",
    "#             logger.info(task_name)\n",
    "            \n",
    "#             test_dataloader = self.get_test_dataloader(test_dataset)\n",
    "#             test_result = self._prediction_loop(\n",
    "#                 test_dataloader, description=\"Prediction\", task_name=task_name, \n",
    "#                 mode=test_dataset.mode)\n",
    "            \n",
    "#             self._log(test_result.metrics)\n",
    "#             for key, value in test_result.metrics.items():\n",
    "#                 logger.info(\"  %s = %s\", key, value)\n",
    "                \n",
    "#             softmax = torch.nn.Softmax(dim=1)\n",
    "#             probs = softmax(torch.Tensor(test_result.predictions)).numpy().astype('float64')\n",
    "#             logits = test_result.predictions.astype('float64')\n",
    "#             output_mode = task_output_modes[task_name] \n",
    "#             if output_mode == \"classification\":\n",
    "#                 predictions = np.argmax(logits, axis=1)\n",
    "            \n",
    "#             self.run_name = wandb.run.name\n",
    "#             output_test_file = os.path.join(\n",
    "#                 self.args.output_dir,\n",
    "#                 f\"{task_name}_test_iter_{self.run_name}.tsv\",\n",
    "#             )\n",
    "#             if scoring_model is None:\n",
    "#                 scoring_model = self.run_name\n",
    "#             if self.is_world_master():\n",
    "#                 with open(output_test_file, \"w\") as writer:\n",
    "#                     logger.info(\"***** Test results {} *****\".format(task_name))\n",
    "#                     logger.info(\"***** Writing as {} *****\".format(self.run_name))\n",
    "#                     if output_mode == \"regression\":\n",
    "#                         writer.write(\"index\\tprediction\\n\")\n",
    "#                     else:\n",
    "#                         writer.write(\"index\\tscoring_model\\tprediction\\tprobability\\tlogits\\n\")\n",
    "#                     for index, item in enumerate(predictions):\n",
    "#                         if output_mode == \"regression\":\n",
    "#                             writer.write(\"%d\\t%3.3f\\n\" % (index, item))\n",
    "#                         else:\n",
    "#                             i_probs = probs[index,:]\n",
    "#                             i_logits = logits[index,:]\n",
    "#                             i_logits = json.dumps(dict(zip(test_dataset.get_labels(), i_logits)))\n",
    "#                             writer.write(\n",
    "#                                 \"%d\\t%s\\t%s\\t%3.6f\\t%s\\n\" % (\n",
    "#                                     index, scoring_model, test_dataset.get_labels()[item], \n",
    "#                                     i_probs[item], i_logits)\n",
    "#                             )\n",
    "                            \n",
    "#     def _prediction_loop(\n",
    "#         self, dataloader: DataLoader, description: str, task_name: str, mode: str,\n",
    "#         prediction_loss_only: Optional[bool] = None, \n",
    "#     ) -> PredictionOutput:\n",
    "#         \"\"\"\n",
    "#         Prediction/evaluation loop, shared by `evaluate()` and `predict()`.\n",
    "#         Works both with or without labels.\n",
    "#         \"\"\"\n",
    "\n",
    "#         prediction_loss_only = prediction_loss_only if prediction_loss_only is not None else self.prediction_loss_only\n",
    "\n",
    "#         model = self.model\n",
    "#         # multi-gpu eval\n",
    "#         if self.args.n_gpu > 1:\n",
    "#             model = torch.nn.DataParallel(model)\n",
    "#         else:\n",
    "#             model = self.model\n",
    "#         # Note: in torch.distributed mode, there's no point in wrapping the model\n",
    "#         # inside a DistributedDataParallel as we'll be under `no_grad` anyways.\n",
    "\n",
    "#         batch_size = dataloader.batch_size\n",
    "#         logger.info(\"***** Running %s *****\", description)\n",
    "#         logger.info(\"  Num examples = %d\", self.num_examples(dataloader))\n",
    "#         logger.info(\"  Batch size = %d\", batch_size)\n",
    "#         eval_losses: List[float] = []\n",
    "#         preds: torch.Tensor = None\n",
    "#         label_ids: torch.Tensor = None\n",
    "#         model.eval()\n",
    "\n",
    "#         if is_tpu_available():\n",
    "#             dataloader = pl.ParallelLoader(dataloader,\n",
    "#                                            [self.args.device]).per_device_loader(self.args.device)\n",
    "\n",
    "#         for inputs in tqdm(dataloader, desc=description):\n",
    "#             has_labels = any(\n",
    "#                 inputs.get(k) is not None for k in [\"labels\", \"lm_labels\", \"masked_lm_labels\"])\n",
    "\n",
    "#             for k, v in inputs.items():\n",
    "#                 inputs[k] = v.to(self.args.device)\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 outputs = model(**inputs)\n",
    "#                 if has_labels:\n",
    "#                     step_eval_loss, logits = outputs[:2]\n",
    "#                     eval_losses += [step_eval_loss.mean().item()]\n",
    "#                 else:\n",
    "#                     logits = outputs[0]\n",
    "\n",
    "#             if not prediction_loss_only:\n",
    "#                 if preds is None:\n",
    "#                     preds = logits.detach()\n",
    "#                 else:\n",
    "#                     preds = torch.cat((preds, logits.detach()), dim=0)\n",
    "#                 if inputs.get(\"labels\") is not None:\n",
    "#                     if label_ids is None:\n",
    "#                         label_ids = inputs[\"labels\"].detach()\n",
    "#                     else:\n",
    "#                         label_ids = torch.cat((label_ids, inputs[\"labels\"].detach()), dim=0)\n",
    "\n",
    "#         if self.args.local_rank != -1:\n",
    "#             # In distributed mode, concatenate all results from all nodes:\n",
    "#             if preds is not None:\n",
    "#                 preds = self.distributed_concat(preds,\n",
    "#                                                 num_total_examples=self.num_examples(dataloader))\n",
    "#             if label_ids is not None:\n",
    "#                 label_ids = self.distributed_concat(label_ids,\n",
    "#                                                     num_total_examples=self.num_examples(dataloader))\n",
    "#         elif is_tpu_available():\n",
    "#             # tpu-comment: Get all predictions and labels from all worker shards of eval dataset\n",
    "#             if preds is not None:\n",
    "#                 preds = xm.mesh_reduce(\"eval_preds\", preds, torch.cat)\n",
    "#             if label_ids is not None:\n",
    "#                 label_ids = xm.mesh_reduce(\"eval_label_ids\", label_ids, torch.cat)\n",
    "\n",
    "#         # Finally, turn the aggregated tensors into numpy arrays.\n",
    "#         if preds is not None:\n",
    "#             preds = preds.cpu().numpy()\n",
    "#         if label_ids is not None:\n",
    "#             label_ids = label_ids.cpu().numpy()\n",
    "\n",
    "#         if self.compute_metrics is not None and preds is not None and label_ids is not None:\n",
    "#             metrics = self.compute_metrics(EvalPrediction(predictions=preds, label_ids=label_ids))\n",
    "#         else:\n",
    "#             metrics = {}\n",
    "#         if len(eval_losses) > 0:\n",
    "#             metrics[f\"{task_name}_{mode}_loss\"] = np.mean(eval_losses)\n",
    "\n",
    "#         # Prefix all keys with {task_name}_{model}_\n",
    "#         for key in list(metrics.keys()):\n",
    "#             if not key.startswith(f\"{task_name}_{mode}_\"):\n",
    "#                 metrics[f\"{task_name}_{mode}_{key}\"] = metrics.pop(key)\n",
    "\n",
    "#         return PredictionOutput(predictions=preds, label_ids=label_ids, metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.encoders.conditional_modules import CBDA\n",
    "from src.utils.misc import Split\n",
    "import math\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 768\n",
    "max_seq_length = 256\n",
    "num_blocks = hidden_size//max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = CaMtlArguments(\n",
    "    model_name_or_path=\"CA-MTL-tiny\", \n",
    "    encoder_type=\"CA-MTL-tiny\")\n",
    "\n",
    "data_args = MultiTaskDataArguments(\n",
    "    data_dir='/hub/CA-MTL/data',\n",
    "    tasks=['D0', 'D1', 'LOC', 'MANC', 'SIGNT'],\n",
    "    overwrite_cache = True,\n",
    "    task_data_folders=[\n",
    "        'D0/2021_04_08', 'D1/2021_04_08', 'MANC/2021_04_08', \n",
    "        'LOC/2021_04_08', 'SIGNT/2021_04_08'])\n",
    "\n",
    "training_args = MultiTaskTrainingArguments(\n",
    "    output_dir='/hub/CA-MTL/mock_models', \n",
    "    overwrite_output_dir=False, \n",
    "    do_train=True, \n",
    "    do_eval=True, \n",
    "    do_predict=True,\n",
    "    evaluate_during_training=True,\n",
    "    per_device_train_batch_size=32, \n",
    "    per_device_eval_batch_size=32, \n",
    "    per_gpu_train_batch_size=None, \n",
    "    per_gpu_eval_batch_size=None, \n",
    "    gradient_accumulation_steps=1, \n",
    "    learning_rate=5e-05, \n",
    "    weight_decay=0.0,\n",
    "    adam_epsilon=1e-08,\n",
    "    max_grad_norm=1.0,\n",
    "    num_train_epochs=7.0, \n",
    "    max_steps=-1, \n",
    "    warmup_steps=0, \n",
    "    logging_dir=None, \n",
    "    logging_first_step=False,\n",
    "    logging_steps=500,\n",
    "    save_steps=1500,\n",
    "    save_total_limit=1, \n",
    "    no_cuda=False, \n",
    "    seed=43, \n",
    "    fp16=False,\n",
    "    fp16_opt_level='O1',\n",
    "    local_rank=-1, \n",
    "    tpu_num_cores=None, \n",
    "    tpu_metrics_debug=False, \n",
    "    use_mt_uncertainty=False, \n",
    "    uniform_mt_sampling=False, \n",
    "    percent_of_max_data_size=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained(CaMtl.get_base_model(model_args.model_name_or_path))\n",
    "\n",
    "model = CaMtl.from_pretrained(\n",
    "    CaMtl.get_base_model(model_args.model_name_or_path),\n",
    "    model_args,\n",
    "    data_args,\n",
    "    config=config)\n",
    "\n",
    "logger.info(model)\n",
    "\n",
    "# load the tokenizer that was used when the model was trained\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    CaMtl.get_base_model(model_args.model_name_or_path),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset=MultiTaskDataset(data_args, tokenizer, limit_length=50)\n",
    "\n",
    "for i, batch in enumerate(train_dataset):\n",
    "    if i == 200:\n",
    "        print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import RandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from transformers import (\n",
    "    DefaultDataCollator,\n",
    ")\n",
    "def get_train_dataloader(train_dataset):\n",
    "    sampler = RandomSampler(train_dataset)\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        sampler=sampler,\n",
    "        batch_size=32,\n",
    "        collate_fn=DefaultDataCollator().collate_batch,\n",
    "    )\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "train_dataloader = get_train_dataloader(train_dataset)\n",
    "for i, batch in enumerate(train_dataloader):\n",
    "    if i == 1:\n",
    "        print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id_2_task_idx = {i: i for i, t in enumerate(data_args.tasks)}\n",
    "def _create_task_type(task_id):\n",
    "    task_type = task_id.clone()\n",
    "    unique_task_ids = torch.unique(task_type)\n",
    "    unique_task_ids_list = (\n",
    "        unique_task_ids.cpu().numpy()\n",
    "        if unique_task_ids.is_cuda\n",
    "        else unique_task_ids.numpy()\n",
    "    )\n",
    "    for unique_task_id in unique_task_ids_list:\n",
    "        task_type[task_type == unique_task_id] = task_id_2_task_idx[\n",
    "            unique_task_id\n",
    "        ]\n",
    "    return task_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = iter(train_dataloader).next()\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_type = _create_task_type(batch['task_id'])\n",
    "\n",
    "task_type_embeddings = nn.Embedding(len(data_args.tasks), hidden_size)\n",
    "task_embedding = task_type_embeddings(task_type)\n",
    "\n",
    "task_transformation = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "task_embedding = task_transformation(task_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_weight_matrix = nn.Parameter(\n",
    "    torch.zeros(\n",
    "        [max_seq_length, math.ceil(max_seq_length/num_blocks)]\n",
    "    ),\n",
    "    requires_grad=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_weight_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_block_diag_attn = CBDA(\n",
    "    hidden_size, math.ceil(max_seq_length/num_blocks), num_blocks\n",
    ")  # d x L/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_scores = cond_block_diag_attn(\n",
    "    x_cond=task_embedding,\n",
    "    x_to_film=random_weight_matrix,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END CBDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -n 1 -r 10\n",
    "# setup_logging(training_args)\n",
    "\n",
    "# set_seed(training_args.seed)\n",
    "# logger.info(training_args)\n",
    "\n",
    "# model_metadata = json.load(open(f\"{model_args.model_name_or_path}/metadata.json\", 'r'))\n",
    "\n",
    "# # update arguments with condition at model training\n",
    "# data_args.max_seq_length = model_metadata['max_seq_length']\n",
    "# num_tasks = len(model_metadata['label_set'])\n",
    "# data_args.task_data_folders = data_args.task_data_folders*num_tasks\n",
    "# data_args.tasks = model_metadata['tasks']\n",
    "# model_args.encoder_type = model_metadata['model_name_or_path']\n",
    "\n",
    "# config = BertConfig.from_pretrained(model_args.model_name_or_path)\n",
    "\n",
    "# model = CaMtl.from_pretrained(\n",
    "#     model_args.model_name_or_path,\n",
    "#     model_args,\n",
    "#     data_args,\n",
    "#     config=config)\n",
    "\n",
    "# logger.info(model)\n",
    "\n",
    "# # load the tokenizer that was used when the model was trained\n",
    "# tokenizer = BertTokenizer.from_pretrained(\n",
    "#     CaMtl.get_base_model(model_metadata['model_name_or_path']),\n",
    "# )\n",
    "\n",
    "# logger.info(\"Training tasks: %s\", \", \".join([t for t in data_args.tasks]))\n",
    "\n",
    "# trainer = MultiTaskTrainer(\n",
    "#     tokenizer,\n",
    "#     data_args,\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=None,\n",
    "#     eval_datasets=None,\n",
    "#     test_datasets=create_eval_datasets(Split.test, data_args, tokenizer, model_metadata)\n",
    "#     if training_args.do_predict\n",
    "#     else None,\n",
    "# )\n",
    "\n",
    "# scoring_model = model_args.model_name_or_path.split(\"/\")[-1]\n",
    "# if training_args.do_predict:\n",
    "#     trainer.predict(scoring_model = scoring_model)\n",
    "# 1min 1s ± 256 ms per loop (mean ± std. dev. of 10 runs, 1 loop each)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ca-mtl-env",
   "language": "python",
   "name": "ca-mtl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
